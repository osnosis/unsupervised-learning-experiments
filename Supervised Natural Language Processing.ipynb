{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "In this exercise we will learn the basics of language parsing and manipulation to try to predict whether a sentence is written by one author or another. Methods we will implement include: bag-of-words, spaCy feature generation, list comprehension, and trying several different classifier models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function for cleaning text, manually remove '--' (spaCy does not recognize). <br>\n",
    "Load and clean data (Alice in Wonderland by Lewis Carroll and Persuasion by Jane Austen).<br>\n",
    "Delete chapter titles.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Parse the cleaned novels into spaCy docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract sentences from spaCy docs. <br>\n",
    "Combine sentences from the two novels into one dataframe for analysis. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                         (I, shall, be, late, !, ')  Carroll"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Time to bag some words! Since spaCy has already tokenized and labelled our data, we can move directly to recording how often various words occur. We will exclude stopwords and punctuation. In addition, in an attempt to keep our feature space from exploding, we will work with lemmas (root words) rather than the raw text terms, and we'll only use the 2000 most common words for each text.\" <br>\n",
    "\n",
    "Create bag-of-words utility function to create a list of the 2000 most common words, filtering out punctuation and stop words. <br>\n",
    "Create function that constructs a dataframe with features for every word in the combined word set. (Each row represents a sentence in either novel. Each column is the count of the times that word appears in the sentence). <br>\n",
    "Scaffold the dataframe and initialize counts to 0. <br>\n",
    "Process each row, counting the occurrence of words in each sentence. <br>\n",
    "Populate the row with word counts for all non-punctuation or stop-word lemmas.<br>\n",
    "Set up the bag-of-words for each novel and combine into a list, then make a dataframe from that combined list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "\n",
    "common_words = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>introduce</th>\n",
       "      <th>sufficient</th>\n",
       "      <th>don't</th>\n",
       "      <th>force</th>\n",
       "      <th>somewhere</th>\n",
       "      <th>dunce</th>\n",
       "      <th>wear</th>\n",
       "      <th>grove</th>\n",
       "      <th>hayters</th>\n",
       "      <th>delight</th>\n",
       "      <th>...</th>\n",
       "      <th>puss</th>\n",
       "      <th>nineteen</th>\n",
       "      <th>renew</th>\n",
       "      <th>leave</th>\n",
       "      <th>overcome</th>\n",
       "      <th>coils</th>\n",
       "      <th>negative</th>\n",
       "      <th>sensation</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  introduce sufficient don't force somewhere dunce wear grove hayters delight  \\\n",
       "0         0          0     0     0         0     0    0     0       0       0   \n",
       "1         0          0     0     0         0     0    0     0       0       0   \n",
       "2         0          0     0     0         0     0    0     0       0       0   \n",
       "3         0          0     0     0         0     0    0     0       0       0   \n",
       "4         0          0     0     0         0     0    0     0       0       0   \n",
       "\n",
       "      ...     puss nineteen renew leave overcome coils negative sensation  \\\n",
       "0     ...        0        0     0     0        0     0        0         0   \n",
       "1     ...        0        0     0     0        0     0        0         0   \n",
       "2     ...        0        0     0     0        0     0        0         0   \n",
       "3     ...        0        0     0     0        0     0        0         0   \n",
       "4     ...        0        0     0     0        0     0        0         0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  \n",
       "1  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
       "3                                      (Oh, dear, !)     Carroll  \n",
       "4                         (I, shall, be, late, !, ')     Carroll  \n",
       "\n",
       "[5 rows x 3004 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Give the bag of words features a whirl by trying a random forest.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.990282131661\n",
      "\n",
      "Test set score: 0.886748120301\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Holy overfitting, Batman! Overfitting is a known problem when using bag of words, since it basically involves throwing a massive number of features at a model – some of those features (in this case, word frequencies) will capture noise in the training set. Since overfitting is also a known problem with Random Forests, the divergence between training score and test score is expected.\" <br>\n",
    "\n",
    "\n",
    "Let's try a technique with some protection against overfitting due to extraneous features – logistic regression with lasso. (Lasso is not specified here...looks like we are using ridge instead!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3190, 3002) (3190,)\n",
      "Training set score: 0.956112852665\n",
      "\n",
      "Test set score: 0.914003759398\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1434       38\n",
      "Carroll         145      511\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "y_pred = train.predict(X_test)\n",
    "cm = pd.crosstab(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression performs a bit better than the random forest. The testing score increases from 0.8867 to **0.914**, and the overfitting is much less prominent. <br>\n",
    "\n",
    "Let's see what gradient boosting can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.876489028213\n",
      "\n",
      "Test set score: 0.86795112782\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a gradient boosting score of **0.8679** using default parameters, it looks like logistic regression is the winner. However, there is always room for improvement! <br>\n",
    "\n",
    "# Same model, new inputs\n",
    "\n",
    "\"What if we feed the model a different novel by Jane Austen, like Emma? Will it be able to distinguish Austen from Carroll with the same level of accuracy if we insert a different sample of Austen's writing?\n",
    "First, we need to process Emma the same way we processed the other data, and combine it with the Alice data:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to\n"
     ]
    }
   ],
   "source": [
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "emma = re.sub(r'VOLUME \\w+', '', emma)\n",
    "emma = re.sub(r'CHAPTER \\w+', '', emma)\n",
    "emma = text_cleaner(emma)\n",
    "print(emma[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emma_doc = nlp(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]\n",
    "\n",
    "emma_sents = emma_sents[0:len(alice_sents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emma is quite long, so we cut it down to the same length as Alice. <br>\n",
    "Build a new Bag of Words dataframe for Emma word counts, and use same common words from Alice and Persuasion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n"
     ]
    }
   ],
   "source": [
    "emma_sentences = pd.DataFrame(emma_sents)\n",
    "emma_bow = bow_features(emma_sentences, common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can model it! Let's use logistic regression again, since it performed the best last time. <br>\n",
    "Combine the Emma sentence data with the Alice data from the test set and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.687173750932\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Austen</th>\n",
       "      <th>Carroll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>1534</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>704</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    Austen  Carroll\n",
       "row_0                   \n",
       "Austen     1534      135\n",
       "Carroll     704      309"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Emma_test = np.concatenate((\n",
    "    X_train[y_train[y_train=='Carroll'].index],\n",
    "    emma_bow.drop(['text_sentence','text_source'], 1)\n",
    "), axis=0)\n",
    "y_Emma_test = pd.concat([y_train[y_train=='Carroll'],\n",
    "                         pd.Series(['Austen'] * emma_bow.shape[0])])\n",
    "\n",
    "print('\\nTest set score:', lr.score(X_Emma_test, y_Emma_test))\n",
    "lr_Emma_predicted = lr.predict(X_Emma_test)\n",
    "pd.crosstab(y_Emma_test, lr_Emma_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Well look at that! NLP approaches are generally effective on the same type of material as they were trained on. It looks like this model is actually able to differentiate multiple works by Austen from Alice in Wonderland. Now the question is whether the model is very good at identifying Austen, or very good at identifying Alice in Wonderland, or both...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 0:\n",
    "\n",
    "Recall that the logistic regression model's best performance on the test set was 93%. See what you can do to improve performance. Suggested avenues of investigation include: Other modeling techniques (SVM?), making more features that take advantage of the spaCy information (include grammar, phrases, POS, etc), making sentence-level features (number of words, amount of punctuation), or including contextual information (length of previous and next sentences, words repeated from one sentence to the next, etc), and anything else your heart desires. Make sure to design your models on the test set, or use cross_validation with multiple folds, and see if you can get accuracy above 97%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will take advantage of spaCy token information to add spaCy features into the sentence dataframe. <br>\n",
    "spaCy features include test, lemma, pos, tag, dep, shape, is_alpha, is_stop, and is_punct. These features are all per token, so I need to gather summary statistics on them per sentence. Since logistic regression with default ridge settings performed best, I will use this again here to evaluate.<br>\n",
    "\n",
    "Using list comprehensions, I will add features for the number of words in the sentence, the average word length, the number of stop words, and the number of quotation marks. These are all features that could potentially differentiate one author's writing style from another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  introduce sufficient don't force somewhere dunce wear grove hayters delight  \\\n",
      "0         0          0     0     0         0     0    0     0       0       0   \n",
      "1         0          0     0     0         0     0    0     0       0       0   \n",
      "2         0          0     0     0         0     0    0     0       0       0   \n",
      "3         0          0     0     0         0     0    0     0       0       0   \n",
      "4         0          0     0     0         0     0    0     0       0       0   \n",
      "\n",
      "     ...     coils negative sensation  \\\n",
      "0    ...         0        0         0   \n",
      "1    ...         0        0         0   \n",
      "2    ...         0        0         0   \n",
      "3    ...         0        0         0   \n",
      "4    ...         0        0         0   \n",
      "\n",
      "                                       text_sentence text_source  \\\n",
      "0  (Alice, was, beginning, to, get, very, tired, ...     Carroll   \n",
      "1  (So, she, was, considering, in, her, own, mind...     Carroll   \n",
      "2  (There, was, nothing, so, VERY, remarkable, in...     Carroll   \n",
      "3                                      (Oh, dear, !)     Carroll   \n",
      "4                         (I, shall, be, late, !, ')     Carroll   \n",
      "\n",
      "  sentence_length avg_word_length num_stop num_tokens num_quotes  \n",
      "0              67        3.656716       37         67          4  \n",
      "1              63        3.730159       32         63          0  \n",
      "2              33        3.393939       18         33          1  \n",
      "3               3        2.333333        0          3          0  \n",
      "4               6        2.333333        1          6          1  \n",
      "\n",
      "[5 rows x 3009 columns]\n"
     ]
    }
   ],
   "source": [
    "def create_spacy_features(df):\n",
    "\n",
    "    df['sentence_length'] = df['text_sentence'].apply(lambda sentence: len(sentence))\n",
    "    df['avg_word_length'] = df['text_sentence'].apply(\n",
    "        lambda sentence: sum(len(word) for word in sentence)/len(sentence))\n",
    "    \n",
    "    def sum_stops (sentence):\n",
    "        sum_stops = []\n",
    "        sum_stops = [sum_stops.append(token) for token in sentence if token.is_stop]\n",
    "        return len(sum_stops)\n",
    "    \n",
    "    def sum_quote (sentence):\n",
    "        sum_quote = []\n",
    "        sum_quote = [sum_quote.append(token) for token in sentence if token.is_quote]\n",
    "        return len(sum_quote)\n",
    "        \n",
    "\n",
    "    df['num_stop'] = df['text_sentence'].apply(lambda sentence: sum_stops(sentence))\n",
    "    df['num_quotes'] = df['text_sentence'].apply(lambda sentence: sum_quote(sentence))\n",
    "        \n",
    "    print(df.head())\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "alice_persuasion_spacy = create_spacy_features(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try iterating through multiple C values for the logistic regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.1\n",
      "Training set score: 0.921630094044\n",
      "Test set score: 0.90507518797\n",
      "\n",
      "\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1411       61\n",
      "Carroll         141      515\n",
      "\n",
      "\n",
      "C = 1\n",
      "Training set score: 0.972727272727\n",
      "Test set score: 0.918703007519\n",
      "\n",
      "\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1416       56\n",
      "Carroll         117      539\n",
      "\n",
      "\n",
      "C = 3\n",
      "Training set score: 0.985579937304\n",
      "Test set score: 0.917293233083\n",
      "\n",
      "\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1411       61\n",
      "Carroll         115      541\n",
      "\n",
      "\n",
      "C = 5\n",
      "Training set score: 0.986833855799\n",
      "Test set score: 0.918233082707\n",
      "\n",
      "\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1409       63\n",
      "Carroll         111      545\n",
      "\n",
      "\n",
      "C = 10\n",
      "Training set score: 0.989655172414\n",
      "Test set score: 0.916353383459\n",
      "\n",
      "\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1402       70\n",
      "Carroll         108      548\n",
      "\n",
      "\n",
      "C = 100\n",
      "Training set score: 0.991222570533\n",
      "Test set score: 0.906484962406\n",
      "\n",
      "\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1381       91\n",
      "Carroll         108      548\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_values = [1e-1, 1, 3, 5, 10, 100]\n",
    "for c in c_values:\n",
    "    run_logistic(alice_persuasion_spacy, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default parameter C=1 gives the highest score at **0.9187.** <br>\n",
    "\n",
    "Let's try using lasso regression, which protects against overfitting due to extraneous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.1\n",
      "Training set score: 0.847962382445\n",
      "Test set score: 0.850093984962\n",
      "\n",
      "\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1380       92\n",
      "Carroll         227      429\n",
      "\n",
      "\n",
      "C = 1\n",
      "Training set score: 0.936677115987\n",
      "Test set score: 0.906015037594\n",
      "\n",
      "\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1412       60\n",
      "Carroll         140      516\n",
      "\n",
      "\n",
      "C = 3\n",
      "Training set score: 0.982131661442\n",
      "Test set score: 0.911654135338\n",
      "\n",
      "\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1408       64\n",
      "Carroll         124      532\n",
      "\n",
      "\n",
      "C = 5\n",
      "Training set score: 0.986206896552\n",
      "Test set score: 0.910714285714\n",
      "\n",
      "\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1408       64\n",
      "Carroll         126      530\n",
      "\n",
      "\n",
      "C = 10\n",
      "Training set score: 0.98934169279\n",
      "Test set score: 0.90977443609\n",
      "\n",
      "\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1398       74\n",
      "Carroll         118      538\n",
      "\n",
      "\n",
      "C = 100\n",
      "Training set score: 0.988087774295\n",
      "Test set score: 0.888157894737\n",
      "\n",
      "\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1364      108\n",
      "Carroll         130      526\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_lasso(df, c):\n",
    "    \n",
    "    Y = df['text_source']\n",
    "    X = df.drop(['text_sentence','text_source'], 1)\n",
    "    \n",
    "    X_dummies = pd.get_dummies(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_dummies, \n",
    "                                                        Y,\n",
    "                                                        test_size=0.4,\n",
    "                                                        random_state=0)\n",
    "    print(f'C = {c}')\n",
    "    lr = LogisticRegression(C=c, penalty='l1', random_state = 1)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print('Training set score:', lr.score(X_train, y_train))\n",
    "    print('Test set score:', lr.score(X_test, y_test))\n",
    "\n",
    "    y_pred = lr.predict(X_test)\n",
    "    cm = pd.crosstab(y_test, y_pred)\n",
    "    print('\\n')\n",
    "    print(cm)\n",
    "    print('\\n')\n",
    "    \n",
    "for c in c_values:\n",
    "    run_lasso(alice_persuasion_spacy, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out lasso regression does not perform as well as ridge, the highest testing score was **0.9116** with default C=1. This is slightly lower than the highest ridge score 0.9187. <br>\n",
    "\n",
    "Next, I will try using a support vector classifier to distinguish between Carroll and Austen's writing styles. I will iterate through several values of the penalty parameter C and pick the best score. I will continue using the spaCy feature set since it produced a better score in logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.001\n",
      "Training set score: 0.682445141066\n",
      "Test set score: 0.691729323308\n",
      "col_0        Austen\n",
      "text_source        \n",
      "Austen         1472\n",
      "Carroll         656\n",
      "\n",
      "\n",
      "C = 0.01\n",
      "Training set score: 0.682445141066\n",
      "Test set score: 0.691729323308\n",
      "col_0        Austen\n",
      "text_source        \n",
      "Austen         1472\n",
      "Carroll         656\n",
      "\n",
      "\n",
      "C = 0.1\n",
      "Training set score: 0.682445141066\n",
      "Test set score: 0.691729323308\n",
      "col_0        Austen\n",
      "text_source        \n",
      "Austen         1472\n",
      "Carroll         656\n",
      "\n",
      "\n",
      "C = 1\n",
      "Training set score: 0.682445141066\n",
      "Test set score: 0.691729323308\n",
      "col_0        Austen\n",
      "text_source        \n",
      "Austen         1472\n",
      "Carroll         656\n",
      "\n",
      "\n",
      "C = 100\n",
      "Training set score: 0.910971786834\n",
      "Test set score: 0.897086466165\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1418       54\n",
      "Carroll         165      491\n",
      "\n",
      "\n",
      "C = 1000\n",
      "Training set score: 0.977115987461\n",
      "Test set score: 0.915883458647\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1404       68\n",
      "Carroll         111      545\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_svc(df, c):\n",
    "    print(f'C = {c}')\n",
    "    \n",
    "    Y = df['text_source']\n",
    "    X = df.drop(['text_sentence','text_source'], 1)\n",
    "    X_dummies = pd.get_dummies(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_dummies, \n",
    "                                                        Y,\n",
    "                                                        test_size=0.4,\n",
    "                                                        random_state=0)\n",
    "    svc = SVC(C = c)\n",
    "    svc.fit(X_train, y_train)\n",
    "    print('Training set score:', svc.score(X_train, y_train))\n",
    "    print('Test set score:', svc.score(X_test, y_test))\n",
    "\n",
    "    y_pred = svc.predict(X_test)\n",
    "    cm = pd.crosstab(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print('\\n')\n",
    "\n",
    "c_values = [1e-3, 1e-2, 1e-1, 1, 100, 1000]\n",
    "for c in c_values:\n",
    "    run_svc(alice_persuasion_spacy, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to a certain value of C, all the training/testing scores are the same, and they are not very good. For these values, the testing score is 0.69. Once C goes up to C=100, the score goes up to 0.88, and at C=1000, the score is **0.9159.** This isn't bad, but doesn't beat the logistic regression score of 0.9187.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some forms of feature selection to see if reducing the features down from ~3000 will help the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 500\n",
      "Training set score: 0.927586206897\n",
      "Test set score: 0.921052631579\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1425       47\n",
      "Carroll         121      535\n",
      "\n",
      "\n",
      "k: 1000\n",
      "Training set score: 0.938871473354\n",
      "Test set score: 0.925281954887\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1425       47\n",
      "Carroll         112      544\n",
      "\n",
      "\n",
      "k: 1500\n",
      "Training set score: 0.943260188088\n",
      "Test set score: 0.926691729323\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1424       48\n",
      "Carroll         108      548\n",
      "\n",
      "\n",
      "k: 2000\n",
      "Training set score: 0.94670846395\n",
      "Test set score: 0.926221804511\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1425       47\n",
      "Carroll         110      546\n",
      "\n",
      "\n",
      "k: 2500\n",
      "Training set score: 0.952664576803\n",
      "Test set score: 0.925281954887\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1424       48\n",
      "Carroll         111      545\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_values = [500,1000,1500,2000,2500]\n",
    "\n",
    "Y = alice_persuasion_spacy['text_source']\n",
    "X = alice_persuasion_spacy.drop(['text_sentence','text_source'], 1)\n",
    "\n",
    "for k in k_values:\n",
    "    print('k:', k)\n",
    "    kb = SelectKBest(k=k)\n",
    "    k_reduced = kb.fit_transform(X,y=Y)\n",
    "\n",
    "    X_train_k, X_test_k, y_train_k, y_test_k = train_test_split(k_reduced, \n",
    "                                                        Y,\n",
    "                                                        test_size=0.4,\n",
    "                                                        random_state=0)\n",
    "\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train_k, y_train_k)\n",
    "    print('Training set score:', lr.score(X_train_k, y_train_k))\n",
    "    print('Test set score:', lr.score(X_test_k, y_test_k))\n",
    "\n",
    "    y_pred = lr.predict(X_test_k)\n",
    "    cm = pd.crosstab(y_test_k, y_pred)\n",
    "    print(cm)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using SelectKBest to reduce the feature set to the 1500 best features, we improve the score from 0.9187 to **0.9267** ! This was achieved using the default value of C=1.<br>\n",
    "\n",
    "Let's try one more method of feature selection before moving on to the next challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5318, 1411)\n",
      "Training set score: 0.954545454545\n",
      "Test set score: 0.924812030075\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1424       48\n",
      "Carroll         112      544\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var = VarianceThreshold(threshold=(.999 * (1 - .999)))\n",
    "var_reduced = var.fit_transform(X)\n",
    "print(var_reduced.shape)\n",
    "\n",
    "X_train_var, X_test_var, y_train_var, y_test_var = train_test_split(var_reduced, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_var, y_train_var)\n",
    "print('Training set score:', lr.score(X_train_var, y_train_var))\n",
    "print('Test set score:', lr.score(X_test_var, y_test_var))\n",
    "\n",
    "y_pred = lr.predict(X_test_var)\n",
    "cm = pd.crosstab(y_test_var, y_pred)\n",
    "print(cm)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a score of **0.9248**, variance threshold produces results that are good, but not better than SelectKBest. <br>\n",
    "\n",
    "Let's see if using k=1500 with SelectKBest can improve the SVC classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 1500\n",
      "C = 0.1\n",
      "Training set score: 0.682445141066\n",
      "Test set score: 0.691729323308\n",
      "col_0        Austen\n",
      "text_source        \n",
      "Austen         1472\n",
      "Carroll         656\n",
      "\n",
      "\n",
      "C = 1\n",
      "Training set score: 0.761128526646\n",
      "Test set score: 0.759868421053\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1388       84\n",
      "Carroll         427      229\n",
      "\n",
      "\n",
      "C = 3\n",
      "Training set score: 0.797178683386\n",
      "Test set score: 0.803571428571\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1373       99\n",
      "Carroll         319      337\n",
      "\n",
      "\n",
      "C = 5\n",
      "Training set score: 0.821943573668\n",
      "Test set score: 0.827537593985\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1382       90\n",
      "Carroll         277      379\n",
      "\n",
      "\n",
      "C = 10\n",
      "Training set score: 0.857053291536\n",
      "Test set score: 0.858082706767\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1389       83\n",
      "Carroll         219      437\n",
      "\n",
      "\n",
      "C = 100\n",
      "Training set score: 0.929467084639\n",
      "Test set score: 0.91212406015\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1413       59\n",
      "Carroll         128      528\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('k: 1500')\n",
    "kb = SelectKBest(k=1500)\n",
    "k_reduced = kb.fit_transform(X,y=Y)\n",
    "\n",
    "for c in c_values:\n",
    "    print(f'C = {c}')\n",
    "    \n",
    "    X_dummies = pd.get_dummies(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(k_reduced, \n",
    "                                                        Y,\n",
    "                                                        test_size=0.4,\n",
    "                                                        random_state=0)\n",
    "    svc = SVC(C = c)\n",
    "    svc.fit(X_train, y_train)\n",
    "    print('Training set score:', svc.score(X_train, y_train))\n",
    "    print('Test set score:', svc.score(X_test, y_test))\n",
    "\n",
    "    y_pred = svc.predict(X_test)\n",
    "    cm = pd.crosstab(y_test, y_pred)\n",
    "    print(cm)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores look good! For some unexplicable reason, C=1000 didn't run so I will make another cell for just that parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 1500\n",
      "C = 1000\n",
      "Training set score: 0.968652037618\n",
      "Test set score: 0.915413533835\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1391       81\n",
      "Carroll          99      557\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('k: 1500')\n",
    "kb = SelectKBest(k=1500)\n",
    "k_reduced = kb.fit_transform(X,y=Y)\n",
    "\n",
    "print(f'C = 1000')\n",
    "\n",
    "X_dummies = pd.get_dummies(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(k_reduced, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "svc = SVC(C = 1000)\n",
    "svc.fit(X_train, y_train)\n",
    "print('Training set score:', svc.score(X_train, y_train))\n",
    "print('Test set score:', svc.score(X_test, y_test))\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "cm = pd.crosstab(y_test, y_pred)\n",
    "print(cm)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better! However, this does not beat our score of 0.9248 using k=1500 with logistic regression. Now, let's move on to the next challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1:\n",
    "\n",
    "Find out whether your new model is good at identifying Alice in Wonderland vs any other work, Persuasion vs any other work, or Austen vs any other work. This will involve pulling a new book from the Project Gutenberg corpus (print(gutenberg.fileids()) for a list) and processing it.\n",
    "Record your work for each challenge in a notebook and submit it below.\n",
    "\n",
    "Let's see whether we can distinguish sentences from another novel. We will use Chesterton's \"The Ballad of the White Horse.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the text and get rid of the chapter titles (denoted with roman numerals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The flying ship of Professor Lucifer sang through the skies like a silver arrow; the bleak white steel of it, gleaming in the bleak blue emptiness of the evening. That it was far above the earth was n\n"
     ]
    }
   ],
   "source": [
    "chesball = gutenberg.raw('chesterton-ball.txt')\n",
    "chesball = re.sub(r'^[IVXLMC]+[.][A-Z ]+$','', chesball, flags=re.MULTILINE)\n",
    "\n",
    "chesball = text_cleaner(chesball)\n",
    "print(chesball[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the text the same way we processed the text above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 5000\n",
      "Processing row 5500\n",
      "Processing row 6000\n",
      "  introduce don't force somewhere dunce wear science delight count  \\\n",
      "0         0     0     0         0     0    0       0       0     0   \n",
      "1         0     0     0         0     0    0       0       0     0   \n",
      "2         0     0     0         0     0    0       0       0     0   \n",
      "3         0     0     0         0     0    0       1       0     0   \n",
      "4         0     0     0         0     0    0       1       0     0   \n",
      "\n",
      "  indescribable     ...     leave bewilder overcome legend negative coils  \\\n",
      "0             0     ...         0        0        0      0        0     0   \n",
      "1             0     ...         0        0        0      0        0     0   \n",
      "2             0     ...         0        0        0      0        0     0   \n",
      "3             0     ...         0        0        0      0        0     0   \n",
      "4             0     ...         0        0        0      0        0     0   \n",
      "\n",
      "  conceivable sensation                                      text_sentence  \\\n",
      "0           0         0  (The, flying, ship, of, Professor, Lucifer, sa...   \n",
      "1           0         0  (That, it, was, far, above, the, earth, was, n...   \n",
      "2           0         0  (The, professor, had, himself, invented, the, ...   \n",
      "3           0         0  (Every, sort, of, tool, or, apparatus, had, ,,...   \n",
      "4           0         0  (For, the, world, of, science, and, evolution,...   \n",
      "\n",
      "  text_source  \n",
      "0  Chesterton  \n",
      "1  Chesterton  \n",
      "2  Chesterton  \n",
      "3  Chesterton  \n",
      "4  Chesterton  \n",
      "\n",
      "[5 rows x 2902 columns]\n"
     ]
    }
   ],
   "source": [
    "chesball_doc = nlp(chesball)\n",
    "\n",
    "chesball_sents = [[sent, \"Chesterton\"] for sent in chesball_doc.sents]\n",
    "ches_al_sentences = pd.DataFrame(chesball_sents + alice_sents)\n",
    "\n",
    "cheswords = bag_of_words(chesball_doc)\n",
    "\n",
    "ches_al_words = set(cheswords + alicewords)\n",
    "\n",
    "ches_al_word_counts = bow_features(ches_al_sentences, ches_al_words)\n",
    "print(ches_al_word_counts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97608\n",
      "34363\n"
     ]
    }
   ],
   "source": [
    "print(len(chesball_doc))\n",
    "print(len(alice_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we may have a bit of class imbalance here. Let's proceed anyways and create the spacy features dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  introduce don't force somewhere dunce wear science delight count  \\\n",
      "0         0     0     0         0     0    0       0       0     0   \n",
      "1         0     0     0         0     0    0       0       0     0   \n",
      "2         0     0     0         0     0    0       0       0     0   \n",
      "3         0     0     0         0     0    0       1       0     0   \n",
      "4         0     0     0         0     0    0       1       0     0   \n",
      "\n",
      "  indescribable    ...     negative coils conceivable sensation  \\\n",
      "0             0    ...            0     0           0         0   \n",
      "1             0    ...            0     0           0         0   \n",
      "2             0    ...            0     0           0         0   \n",
      "3             0    ...            0     0           0         0   \n",
      "4             0    ...            0     0           0         0   \n",
      "\n",
      "                                       text_sentence text_source  \\\n",
      "0  (The, flying, ship, of, Professor, Lucifer, sa...  Chesterton   \n",
      "1  (That, it, was, far, above, the, earth, was, n...  Chesterton   \n",
      "2  (The, professor, had, himself, invented, the, ...  Chesterton   \n",
      "3  (Every, sort, of, tool, or, apparatus, had, ,,...  Chesterton   \n",
      "4  (For, the, world, of, science, and, evolution,...  Chesterton   \n",
      "\n",
      "  sentence_length avg_word_length num_stop num_quotes  \n",
      "0              32        4.187500       11          0  \n",
      "1              29        3.137931       19          0  \n",
      "2              18        4.777778        9          0  \n",
      "3              28        4.214286       13          0  \n",
      "4              57        4.280702       33          0  \n",
      "\n",
      "[5 rows x 2906 columns]\n"
     ]
    }
   ],
   "source": [
    "ches_al_spacy = create_spacy_features(ches_al_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.1\n",
      "Training set score: 0.900476442562\n",
      "Test set score: 0.877380952381\n",
      "\n",
      "\n",
      "col_0        Carroll  Chesterton\n",
      "text_source                     \n",
      "Carroll          389         264\n",
      "Chesterton        45        1822\n",
      "\n",
      "\n",
      "C = 1\n",
      "Training set score: 0.965854949709\n",
      "Test set score: 0.906746031746\n",
      "\n",
      "\n",
      "col_0        Carroll  Chesterton\n",
      "text_source                     \n",
      "Carroll          470         183\n",
      "Chesterton        52        1815\n",
      "\n",
      "\n",
      "C = 3\n",
      "Training set score: 0.979354155638\n",
      "Test set score: 0.905952380952\n",
      "\n",
      "\n",
      "col_0        Carroll  Chesterton\n",
      "text_source                     \n",
      "Carroll          480         173\n",
      "Chesterton        64        1803\n",
      "\n",
      "\n",
      "C = 5\n",
      "Training set score: 0.981206987824\n",
      "Test set score: 0.903571428571\n",
      "\n",
      "\n",
      "col_0        Carroll  Chesterton\n",
      "text_source                     \n",
      "Carroll          479         174\n",
      "Chesterton        69        1798\n",
      "\n",
      "\n",
      "C = 10\n",
      "Training set score: 0.983589200635\n",
      "Test set score: 0.900793650794\n",
      "\n",
      "\n",
      "col_0        Carroll  Chesterton\n",
      "text_source                     \n",
      "Carroll          481         172\n",
      "Chesterton        78        1789\n",
      "\n",
      "\n",
      "C = 100\n",
      "Training set score: 0.985706723134\n",
      "Test set score: 0.887301587302\n",
      "\n",
      "\n",
      "col_0        Carroll  Chesterton\n",
      "text_source                     \n",
      "Carroll          485         168\n",
      "Chesterton       116        1751\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in c_values:\n",
    "    run_logistic(ches_al_spacy, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Identifying Lewis Carroll vs. G.K. Chesterton using logistic regression gives us a testing accuracy score of **0.9067**. Let's see what happens when we reduce features using SelectKBest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 1500\n",
      "Training set score: 0.943260188088\n",
      "Test set score: 0.926691729323\n",
      "col_0        Austen  Carroll\n",
      "text_source                 \n",
      "Austen         1424       48\n",
      "Carroll         108      548\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y = alice_persuasion_spacy['text_source']\n",
    "X = alice_persuasion_spacy.drop(['text_sentence','text_source'], 1)\n",
    "\n",
    "print('k: 1500')\n",
    "kb = SelectKBest(k=1500)\n",
    "k_reduced = kb.fit_transform(X,y=Y)\n",
    "\n",
    "X_train_k, X_test_k, y_train_k, y_test_k = train_test_split(k_reduced, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_k, y_train_k)\n",
    "print('Training set score:', lr.score(X_train_k, y_train_k))\n",
    "print('Test set score:', lr.score(X_test_k, y_test_k))\n",
    "\n",
    "y_pred = lr.predict(X_test_k)\n",
    "cm = pd.crosstab(y_test_k, y_pred)\n",
    "print(cm)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using the 1500 best features allows us to increase the score from 0.9067 to **0.9267.** This is our best score yet! Now let's run SVC, which performed the best for the last dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.1\n",
      "Training set score: 0.731074642668\n",
      "Test set score: 0.740873015873\n",
      "col_0        Chesterton\n",
      "text_source            \n",
      "Carroll             653\n",
      "Chesterton         1867\n",
      "\n",
      "\n",
      "C = 1\n",
      "Training set score: 0.731604023293\n",
      "Test set score: 0.741666666667\n",
      "col_0        Carroll  Chesterton\n",
      "text_source                     \n",
      "Carroll            4         649\n",
      "Chesterton         2        1865\n",
      "\n",
      "\n",
      "C = 3\n",
      "Training set score: 0.732662784542\n",
      "Test set score: 0.74246031746\n",
      "col_0        Carroll  Chesterton\n",
      "text_source                     \n",
      "Carroll            6         647\n",
      "Chesterton         2        1865\n",
      "\n",
      "\n",
      "C = 5\n",
      "Training set score: 0.733456855479\n",
      "Test set score: 0.742857142857\n",
      "col_0        Carroll  Chesterton\n",
      "text_source                     \n",
      "Carroll            8         645\n",
      "Chesterton         3        1864\n",
      "\n",
      "\n",
      "C = 10\n",
      "Training set score: 0.744044467972\n",
      "Test set score: 0.75\n",
      "col_0        Carroll  Chesterton\n",
      "text_source                     \n",
      "Carroll           26         627\n",
      "Chesterton         3        1864\n",
      "\n",
      "\n",
      "C = 100\n",
      "Training set score: 0.862625727898\n",
      "Test set score: 0.855555555556\n",
      "col_0        Carroll  Chesterton\n",
      "text_source                     \n",
      "Carroll          292         361\n",
      "Chesterton         3        1864\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in c_values:\n",
    "    run_svc(ches_al_spacy, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, these results are much worse than all of the other methods. This looks like a case of class imbalance, as the classifier is aggressively predicting Chesterton, which is the longer document. In the interest of time, I will not re-run these. But, in the future, I would randomly sub-sample the larger class (Chesterton) to create two equally sized classes then re-run this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "When classes are comparably sized, logistic regression with feature reduction by SelectKBest likely gives the best results. Although SVC is a very powerful classifier, like anything else it requires some fine-tuning and parameter optimizing, and this takes too long to run for this simple drill.<br>\n",
    "\n",
    "The class imbalance does not seem to affect the accuracy for logistic regression, which is a testament to this method's robustness and versatility compared to SVC. However, I acknowledge that it is good practice to represent the classes equally going forth."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
