{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network RBM for Digit Recognition\n",
    "\n",
    "Drill: Use RBM to perform feature extraction on an image-based dataset that you find or create. If you go this route, present the features you extract and explain why this is a useful feature extraction method in the context you’re operating in. DO NOT USE either the MNIST digit recognition database or the iris data set. They’ve been worked on in very public ways very very many times and the code is easily available. (However, that code could be a useful resource to refer to)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to use a dataset from Kaggle that has a collection of handwritten black-and-white digits. Using a neural network, I will to predict what digit each pictoral datapoint is supposed to represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "1       0    ...            0         0         0         0         0   \n",
      "2       0    ...            0         0         0         0         0   \n",
      "3       0    ...            0         0         0         0         0   \n",
      "4       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0         0  \n",
      "1         0         0         0         0         0  \n",
      "2         0         0         0         0         0  \n",
      "3         0         0         0         0         0  \n",
      "4         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n",
      "(42000, 785)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define: <br>\n",
    "X = all of the feature columns. <br>\n",
    "Y = the digit output. <br>\n",
    "Split X and Y into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df.drop('label', axis=1)\n",
    "Y = df['label']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline that incorporates a Restricted Boltzmann Machine neural network with a logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rbm = BernoulliRBM(random_state=0, verbose=True)\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "pipe = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the possible hyperparameters for the rbm and the logistic classifiers and iterate through a GridSearch to find the best permutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rates = [1e-3, 1e-2, 1e-1, 0.25, 0.5, 1]\n",
    "n_iters = [20]\n",
    "components = [10, 50, 100, 300]\n",
    "c_values = [1e-3, 1e-2, 1e-1, 0.25, 0.5, 1, 100, 1000]\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'rbm__learning_rate': learning_rates,\n",
    "        'rbm__n_iter': n_iters,\n",
    "        'rbm__n_components': components,\n",
    "        'logistic__C': c_values\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display best parameters. <br>\n",
    "This section has been commented out because it takes too long to run, best params from a previous run are recorded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#grid = GridSearchCV(pipe, cv=5, n_jobs=1, param_grid=param_grid)\n",
    "#grid.fit(X_train, Y_train)\n",
    "\n",
    "#print(f'best params:\\n {grid.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Params\n",
    "\n",
    "logistic C: 0.001 <br>\n",
    "rbm learning_rate: 0.001 <br>\n",
    "rbm n_components: 10 <br>\n",
    "rbm n_iter: 20 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-fit the pipeline model with these newly determined parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -437048.62, time = 2.74s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -873982.26, time = 2.90s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -1310915.50, time = 3.28s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -1747849.14, time = 2.83s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -2184781.93, time = 3.86s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -2621714.79, time = 4.50s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -3058647.89, time = 4.22s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -3495581.98, time = 2.94s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -3932514.89, time = 2.88s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -4369448.00, time = 2.89s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -4806380.79, time = 2.88s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -5243314.08, time = 2.87s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -5680247.51, time = 2.71s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -6117181.60, time = 3.03s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -6554114.34, time = 2.83s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -6991047.04, time = 2.84s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -7427980.52, time = 2.91s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -7864914.24, time = 2.86s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -8301847.32, time = 2.76s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -8738779.94, time = 2.82s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('rbm', BernoulliRBM(batch_size=10, learning_rate=0.001, n_components=10, n_iter=20,\n",
       "       random_state=0, verbose=True)), ('logistic', LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm.learning_rate = 0.001\n",
    "rbm.n_components = 10\n",
    "rbm.n_iter = 20\n",
    "logistic.C = 0.001\n",
    "\n",
    "pipe.set_params(rbm__learning_rate=0.001, rbm__n_components=10,rbm__n_iter=20,logistic__C=0.001).fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.11080357142857143\n",
      "Testing Accuracy: 0.1144047619047619\n",
      "Logistic regression using RBM features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       813\n",
      "          1       0.11      1.00      0.21       961\n",
      "          2       0.00      0.00      0.00       860\n",
      "          3       0.00      0.00      0.00       863\n",
      "          4       0.00      0.00      0.00       827\n",
      "          5       0.00      0.00      0.00       756\n",
      "          6       0.00      0.00      0.00       841\n",
      "          7       0.00      0.00      0.00       899\n",
      "          8       0.00      0.00      0.00       768\n",
      "          9       0.00      0.00      0.00       812\n",
      "\n",
      "avg / total       0.01      0.11      0.02      8400\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "train_pred = pipe.predict(X_train)\n",
    "test_pred = pipe.predict(X_test)\n",
    "print(f'Training Accuracy: {accuracy_score(Y_train, train_pred)}')\n",
    "print(f'Testing Accuracy: {accuracy_score(Y_test, test_pred)}')\n",
    "\n",
    "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
    "    classification_report(\n",
    "        Y_test,\n",
    "        pipe.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! This score is awful! Let's see how the score compares to using logistic regression without an RBM to transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9377678571428572\n",
      "Testing Accuracy: 0.9085714285714286\n",
      "Logistic regression using raw pixel features:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.95      0.95       813\n",
      "          1       0.96      0.97      0.96       961\n",
      "          2       0.91      0.88      0.90       860\n",
      "          3       0.89      0.88      0.89       863\n",
      "          4       0.92      0.91      0.91       827\n",
      "          5       0.87      0.83      0.85       756\n",
      "          6       0.94      0.96      0.95       841\n",
      "          7       0.93      0.92      0.93       899\n",
      "          8       0.83      0.89      0.86       768\n",
      "          9       0.87      0.88      0.87       812\n",
      "\n",
      "avg / total       0.91      0.91      0.91      8400\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_classifier = linear_model.LogisticRegression(C=0.001)\n",
    "logistic_classifier.fit(X_train, Y_train)\n",
    "\n",
    "train_pred = logistic_classifier.predict(X_train)\n",
    "test_pred = logistic_classifier.predict(X_test)\n",
    "print(f'Training Accuracy: {accuracy_score(Y_train, train_pred)}')\n",
    "print(f'Testing Accuracy: {accuracy_score(Y_test, test_pred)}')\n",
    "\n",
    "print(\"Logistic regression using raw pixel features:\\n%s\\n\" % (\n",
    "    classification_report(\n",
    "        Y_test,\n",
    "        logistic_classifier.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With a testing score of **0.908**, logistic regression on the original dataset performed much better than logistic regression on the RBM transformed dataset, which had a testing score of **0.114.** Why was the RBM model so much worse? It is possible that this dataset simply isn't suited for a restricted boltzmann machine. RBM does not work on every dataset, and there are other methods that can be used as alternatives such as SGDClassifier or PassiveAggressiveClassifier. <br>\n",
    "\n",
    "Additionally, I had to limit my program to n_iter = 20 iterations due to memory constraints, which is still a very low number, and the program still took a week to run. The example that I based this algorithm off had n_iter = 20 but this hyperparameter is often in the range of 50-100, and I have even seen as high as 2000. n_iter represents the number of iterations/sweeps over the training dataset to perform during training, and the more sweeps you can do, the more thorough grasp you have on the data. If I had a stronger machine, I would increase the n_iter and see how that changes the data, but RBM may still not be the best method here.\n",
    "\n",
    "Another note is that this particular digit dataset is extremely messy, and much harder to analyze that the dataset in the example. Despite this underperforming RBM model, this exercise was useful in getting me to optimize RBM models with time constraints, and implement pipeline and grid search techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
