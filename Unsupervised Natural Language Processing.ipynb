{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drill\n",
    "\n",
    "In this experiment, we will tinker with ways we can cluster or group paragraphs by similarity. We will see how changing different parameters and features change the similarity of our sentence groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data in the form of paragraphs. <br>\n",
    "To clean, remove double-dash from all words (spaCy does not recognize). <br>\n",
    "(We will actually be using the first sentence per paragraph to cut down our processing time.) <br>\n",
    "Form each paragraph into a string and add it to the list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[ Emma by Jane Austen 1816 ]', 'VOLUME I', 'CHAPTER I', 'Emma Woodhouse , handsome , clever , and rich , with a comfortable home and happy disposition , seemed to unite some of the best blessings of existence ; and had lived nearly twenty - one years in the world with very little to distress or vex her .']\n"
     ]
    }
   ],
   "source": [
    "emma=gutenberg.paras('austen-emma.txt')\n",
    "\n",
    "emma_paras=[]\n",
    "for paragraph in emma:\n",
    "    para=paragraph[0]\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    emma_paras.append(' '.join(para))\n",
    "\n",
    "print(emma_paras[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the sklean tfidf function. We will be doing dimension reduction later, so let's keep all the words for now. <br>\n",
    "\n",
    "max_df: drops all words that have a document frequency higher than this specific threshold.<br>\n",
    "min_df: uses all words that appear at least that many times <br>\n",
    "stop_words: defines the dictionary of stop words to pull from <br>\n",
    "lowercase: converts all words to lowercase since Alice in Wonderland has a habit of capitalizing <br>\n",
    "use_idf: use inverse document frequencies in the weighting <br>\n",
    "norm: (u'l2') applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally <br>\n",
    "smooth_idf: adds 1 to all document frequencies to prevent divide-by-zero errors <br><br>\n",
    "\n",
    "Apply the vectorizer to create new vectorized dataframe. <br>\n",
    "Split into training and testing sets. <br>\n",
    "Convert to csr to reshape the vectorizer output into something people can read. <br><br>\n",
    "\n",
    "Define n as the number of paragraphs (number of rows in the dataframe). <br>\n",
    "Create a list of dictionaries; one dictionary per paragraph. <br>\n",
    "Define terms as the list of features using sklearn built-in feature get_feature_names(). <br>\n",
    "For each paragraph, list the feature words and their tf-idf scores. Calling .nonzero() returns a tuple of arrays, one for each dimension of a, containing the indices of the non-zero elements in that dimension. Because so many of the values are 0, we don't need to waste space creating a dataframe of mostly 0's. This returns the positions of the non-zero values.<br>\n",
    "Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1948\n",
      "emma_paras_tfidf\n",
      "  (0, 561)\t0.526920649962\n",
      "  (0, 959)\t0.849914483136\n",
      "  (1, 1859)\t1.0\n",
      "  (2, 274)\t1.0\n",
      "  (3, 561)\t0.0981254774043\n",
      "  (3, 1914)\t0.151764777883\n",
      "  (3, 807)\t0.23491932626\n",
      "  (3, 307)\t0.23491932626\n",
      "  (3, 1474)\t0.272205137645\n",
      "  (3, 323)\t0.226030521534\n",
      "  (3, 853)\t0.192257673953\n",
      "  (3, 816)\t0.18642581838\n",
      "  (3, 504)\t0.238432290064\n",
      "  (3, 1815)\t0.282808587453\n",
      "  (3, 199)\t0.211085798403\n",
      "  (3, 205)\t0.282808587453\n",
      "  (3, 624)\t0.282808587453\n",
      "  (3, 1038)\t0.263980463003\n",
      "  (3, 1157)\t0.246656964706\n",
      "  (3, 1941)\t0.218874275973\n",
      "  (3, 1918)\t0.20773274037\n",
      "  (3, 1037)\t0.151764777883\n",
      "  (3, 511)\t0.272205137645\n",
      "  (4, 1946)\t0.331370087052\n",
      "  (4, 433)\t0.318945902492\n",
      "  :\t:\n",
      "  (2368, 857)\t0.161541253151\n",
      "  (2368, 241)\t0.206332022994\n",
      "  (2368, 691)\t0.248482533856\n",
      "  (2368, 1483)\t0.225160877248\n",
      "  (2368, 961)\t0.217652980766\n",
      "  (2368, 1131)\t0.240974637375\n",
      "  (2368, 374)\t0.248482533856\n",
      "  (2368, 1319)\t0.258161895894\n",
      "  (2369, 1037)\t0.163664149393\n",
      "  (2369, 1142)\t0.131642037081\n",
      "  (2369, 879)\t0.249880027831\n",
      "  (2369, 1751)\t0.187255367441\n",
      "  (2369, 1880)\t0.277431348044\n",
      "  (2369, 1025)\t0.192367670658\n",
      "  (2369, 559)\t0.150289999437\n",
      "  (2369, 1235)\t0.265996517001\n",
      "  (2369, 236)\t0.220706108732\n",
      "  (2369, 1725)\t0.257126972973\n",
      "  (2369, 989)\t0.293547837215\n",
      "  (2369, 830)\t0.212155828694\n",
      "  (2369, 641)\t0.227636333848\n",
      "  (2369, 916)\t0.293547837215\n",
      "  (2369, 1891)\t0.304982668258\n",
      "  (2369, 1225)\t0.304982668258\n",
      "  (2369, 1234)\t0.284678293187\n",
      "emma_paras_csr\n",
      "  (1, 1610)\t0.557269551945\n",
      "  (1, 1184)\t0.410435708677\n",
      "  (1, 1942)\t0.337101374819\n",
      "  (1, 975)\t0.416027723484\n",
      "  (1, 1585)\t0.379840256896\n",
      "  (1, 478)\t0.299996918561\n",
      "  (2, 212)\t0.387310339852\n",
      "  (2, 923)\t0.387310339852\n",
      "  (2, 709)\t0.396057459414\n",
      "  (2, 1278)\t0.379733240238\n",
      "  (2, 236)\t0.315077230161\n",
      "  (2, 298)\t0.35218360557\n",
      "  (2, 1412)\t0.419065154112\n",
      "  (3, 24)\t0.158938756421\n",
      "  (3, 1543)\t0.148508532741\n",
      "  (3, 1462)\t0.154892714006\n",
      "  (3, 552)\t0.151471833888\n",
      "  (3, 647)\t0.163890712125\n",
      "  (3, 123)\t0.17027489339\n",
      "  (3, 1423)\t0.17027489339\n",
      "  (3, 479)\t0.17027489339\n",
      "  (3, 860)\t0.17027489339\n",
      "  (3, 1540)\t0.163890712125\n",
      "  (3, 1527)\t0.163890712125\n",
      "  (3, 422)\t0.154892714006\n",
      "  :\t:\n",
      "  (1421, 126)\t0.215085129736\n",
      "  (1421, 1479)\t0.227579814235\n",
      "  (1421, 581)\t0.206220010002\n",
      "  (1421, 1087)\t0.227579814235\n",
      "  (1421, 1797)\t0.206220010002\n",
      "  (1421, 639)\t0.196406640569\n",
      "  (1421, 1207)\t0.199343688917\n",
      "  (1421, 738)\t0.220703493151\n",
      "  (1421, 1710)\t0.220703493151\n",
      "  (1421, 1266)\t0.215085129736\n",
      "  (1421, 397)\t0.220703493151\n",
      "  (1421, 627)\t0.182992016601\n",
      "  (1421, 38)\t0.196406640569\n",
      "  (1421, 1126)\t0.152994515685\n",
      "  (1421, 811)\t0.202590445236\n",
      "  (1421, 820)\t0.216894052683\n",
      "  (1421, 970)\t0.152994515685\n",
      "  (1421, 1632)\t0.175046836335\n",
      "  (1421, 722)\t0.13684491415\n",
      "  (1421, 1046)\t0.159870836769\n",
      "  (1421, 295)\t0.138942028144\n",
      "  (1421, 783)\t0.138085981884\n",
      "  (1421, 823)\t0.157404266012\n",
      "  (1421, 868)\t0.159870836769\n",
      "  (1421, 1037)\t0.126884452869\n",
      "Original sentence: A very few minutes more , however , completed the present trial .\n",
      "Tf_idf vector: {'minutes': 0.71274503103825837, 'present': 0.70142321085794701}\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(emma_paras, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5,\n",
    "                             min_df=2,\n",
    "                             stop_words='english', \n",
    "                             lowercase=True,\n",
    "                             use_idf=True,\n",
    "                             norm=u'l2',\n",
    "                             smooth_idf=True\n",
    "                            )\n",
    "\n",
    "emma_paras_tfidf=vectorizer.fit_transform(emma_paras)\n",
    "print(\"Number of features: %d\" % emma_paras_tfidf.get_shape()[1])\n",
    "print('emma_paras_tfidf')\n",
    "print(emma_paras_tfidf)\n",
    "\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(emma_paras_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "print('emma_paras_csr')\n",
    "print(X_train_tfidf_csr)\n",
    "\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension reduction\n",
    "Okay, now we have our vectors, with one vector per paragraph. It's time to do some dimension reduction. We use the Singular Value Decomposition (SVD) function from sklearn rather than PCA because we don't want to mean-center our variables (and thus lose sparsity). <br>\n",
    "Reduce the feature space from 1379 to 130. <br>\n",
    "Run SVD on the training data then project. <br>\n",
    "Look at what sorts of paragraphs (sentences) our solution considers similar, for the first five identified topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 45.20116821\n",
      "Component 0:\n",
      "\" Oh !\"    0.999292\n",
      "\" Oh !     0.999292\n",
      "\" Oh !     0.999292\n",
      "\" Oh !     0.999292\n",
      "\" Oh !     0.999292\n",
      "\" Oh !\"    0.999292\n",
      "\" Oh !     0.999292\n",
      "\" Oh !\"    0.999292\n",
      "\" Oh !     0.999292\n",
      "\" Oh !     0.999292\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "\" You have made her too tall , Emma ,\" said Mr . Knightley .                                                                                                                0.634400\n",
      "\" You get upon delicate subjects , Emma ,\" said Mrs . Weston smiling ; \" remember that I am here . Mr .                                                                     0.590476\n",
      "\" You are right , Mrs . Weston ,\" said Mr . Knightley warmly , \" Miss Fairfax is as capable as any of us of forming a just opinion of Mrs . Elton .                         0.567739\n",
      "\" I do not know what your opinion may be , Mrs . Weston ,\" said Mr . Knightley , \" of this great intimacy between Emma and Harriet Smith , but I think it a bad thing .\"    0.562058\n",
      "\" There were misunderstandings between them , Emma ; he said so expressly .                                                                                                 0.528623\n",
      "Mr . Knightley might quarrel with her , but Emma could not quarrel with herself .                                                                                           0.526672\n",
      "Emma found that it was not Mr . Weston ' s fault that the number of privy councillors was not yet larger .                                                                  0.508661\n",
      "\" Now ,\" said Emma , when they were fairly beyond the sweep gates , \" now Mr . Weston , do let me know what has happened .\"                                                 0.504876\n",
      "\" In one respect , perhaps , Mr . Elton ' s manners are superior to Mr . Knightley ' s or Mr . Weston ' s .                                                                 0.504776\n",
      "\" I do not admire it ,\" said Mr . Knightley .                                                                                                                               0.499206\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "CHAPTER X       0.998677\n",
      "CHAPTER I       0.998677\n",
      "CHAPTER V       0.998677\n",
      "CHAPTER X       0.998677\n",
      "CHAPTER I       0.998677\n",
      "CHAPTER I       0.998677\n",
      "CHAPTER X       0.998677\n",
      "CHAPTER V       0.998677\n",
      "CHAPTER V       0.998677\n",
      "CHAPTER XVII    0.997657\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "\" Ah !      0.992915\n",
      "\" Ah !      0.992915\n",
      "But ah !    0.992915\n",
      "\" Ah !      0.992915\n",
      "\" Ah !      0.992915\n",
      "\" Ah !      0.992915\n",
      "But ah !    0.992915\n",
      "\" Ah !\"     0.992915\n",
      "\" Ah !      0.992915\n",
      "\" Ah !      0.992915\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "\" There were misunderstandings between them , Emma ; he said so expressly .    0.650924\n",
      "Emma demurred .                                                                0.599412\n",
      "\" Are you well , my Emma ?\"                                                    0.599412\n",
      "Emma was silenced .                                                            0.589000\n",
      "At first it was downright dulness to Emma .                                    0.588316\n",
      "\" Emma , my dear Emma \"                                                        0.577285\n",
      "Emma could not resist .                                                        0.572860\n",
      "\" It is not now worth a regret ,\" said Emma .                                  0.551938\n",
      "\" For shame , Emma !                                                           0.545978\n",
      "\" I am ready ,\" said Emma , \" whenever I am wanted .\"                          0.496786\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From gazing at the most representative sample paragraphs, it appears that component 0 targets the exclamation 'Oh!', component 1 seems to largely involve critical dialogue directed at or about the main character Emma, component 2 is chapter headings, component 3 is exclamations involving 'Ah!, and component 4 involves actions by or directly related to Emma. What fun!\n",
    "\n",
    "# Sentence similarity\n",
    "\n",
    "We can also look at how similar various sentences are to one another. For example, here are the similarity scores (as a heatmap) of the first 10 sentences in the training set: <br>\n",
    "\n",
    "Compute document similarity using LSA components. <br>\n",
    "Filter down to only first 10 sentences. <br>\n",
    "Make a heat map and generate a key. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD3CAYAAAAjdY4DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8flIGnnJlQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1198399e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:\n",
      "0 That is _court_ .\n",
      "1 \" Yes , sir , I did indeed ; and I am very much obliged by your kind solicitude about me .\"\n",
      "2 \" How much his business engrosses him already is very plain from the circumstance of his forgetting to inquire for the book you recommended .\n",
      "3 To restrain him as much as might be , by her own manners , she was immediately preparing to speak with exquisite calmness and gravity of the weather and the night ; but scarcely had she begun , scarcely had they passed the sweep - gate and joined the other carriage , than she found her subject cut up  her hand seized  her attention demanded , and Mr . Elton actually making violent love to her : availing himself of the precious opportunity , declaring sentiments which must be already well known , hoping  fearing  adoring  ready to die if she refused him ; but flattering himself that his ardent attachment and unequalled love and unexampled passion could not fail of having some effect , and in short , very much resolved on being seriously accepted as soon as possible .\n",
      "4 Emma smiled and answered \" My visit was of use to the nervous part of her complaint , I hope ; but not even I can charm away a sore throat ; it is a most severe cold indeed .\n",
      "5 A very few minutes more , however , completed the present trial .\n",
      "6 \" I am delighted to hear you speak so stoutly on the subject ,\" replied Emma , smiling ; \" but you do not mean to deny that there was a time  and not very distant either  when you gave me reason to understand that you did care about him ?\"\n",
      "7 \" Very well ; and if he had intended to give her one , he would have told her so .\"\n",
      "8 Some laughed , and answered good - humouredly .\n",
      "9 \" There appeared such a perfectly good understanding among them all \" he began rather quickly , but checking himself , added , \" however , it is impossible for me to say on what terms they really were  how it might all be behind the scenes .\n"
     ]
    }
   ],
   "source": [
    "similarity = np.asarray(np.asmatrix(X_train_lsa) * np.asmatrix(X_train_lsa).T)\n",
    "sim_matrix=pd.DataFrame(similarity,index=X_train).iloc[0:10,0:10]\n",
    "ax = sns.heatmap(sim_matrix,yticklabels=range(10))\n",
    "plt.show()\n",
    "\n",
    "print('Key:')\n",
    "for i in range(10):\n",
    "    print(i,sim_matrix.index[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much similarity at all except between sentences 8 and 9, both of which seem to describe people getting along well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drill 0: Test set\n",
    "\n",
    "Now it's your turn: Apply our LSA model to the test set. Does it identify similar sentences for components 0 through 4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 0:\n",
      "\" Oh !    0.999286\n",
      "\" Oh !    0.999286\n",
      "\" Oh !    0.999286\n",
      "\" Oh !    0.999286\n",
      "\" Oh !    0.999286\n",
      "\" Oh !    0.999286\n",
      "\" Oh !    0.999286\n",
      "\" Oh !    0.999286\n",
      "\" Oh !    0.999286\n",
      "\" Oh !    0.999286\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "\" Well , Mrs . Weston ,\" said Emma triumphantly when he left them , \" what do you say now to Mr . Knightley ' s marrying Jane Fairfax ?\"                                                                                                                                                                                                                                                                                                                                                                          0.674153\n",
      "Frank turned instantly to Emma , to claim her former promise ; and boasted himself an engaged man , which his father looked his most perfect approbation of  and it then appeared that Mrs . Weston was wanting _him_ to dance with Mrs . Elton himself , and that their business was to help to persuade him into it , which was done pretty soon . Mr . Weston and Mrs . Elton led the way , Mr . Frank Churchill and Miss Woodhouse followed .                                                                 0.601843\n",
      "After tea , Mr . and Mrs . Weston , and Mr . Elton sat down with Mr . Woodhouse to cards .                                                                                                                                                                                                                                                                                                                                                                                                                        0.565565\n",
      "\" He is a person I never think of from one month ' s end to another ,\" said Mr . Knightley , with a degree of vexation , which made Emma immediately talk of something else , though she could not comprehend why he should be angry .                                                                                                                                                                                                                                                                            0.564743\n",
      "In this walk Emma and Mr . Weston found all the others assembled ; and towards this view she immediately perceived Mr . Knightley and Harriet distinct from the rest , quietly leading the way .                                                                                                                                                                                                                                                                                                                  0.564363\n",
      "The result of this distress was , that , with a much more voluntary , cheerful consent than his daughter had ever presumed to hope for at the moment , she was able to fix her wedding - day  and Mr . Elton was called on , within a month from the marriage of Mr . and Mrs . Robert Martin , to join the hands of Mr . Knightley and Miss Woodhouse .                                                                                                                                                          0.556759\n",
      "\" Mrs . Weston ' s manners ,\" said Emma , \" were always particularly good .                                                                                                                                                                                                                                                                                                                                                                                                                                       0.542707\n",
      "Emma was more than half in hopes of Mr . Elton ' s having dropt a hint .                                                                                                                                                                                                                                                                                                                                                                                                                                          0.541883\n",
      "He had frightened her a little about Mr . Elton ; but when she considered that Mr . Knightley could not have observed him as she had done , neither with the interest , nor ( she must be allowed to tell herself , in spite of Mr . Knightley ' s pretensions ) with the skill of such an observer on such a question as herself , that he had spoken it hastily and in anger , she was able to believe , that he had rather said what he wished resentfully to be true , than what he knew any thing about .    0.534332\n",
      "\" Well ,\" said Emma , \" there is no disputing about taste . At least you admire her except her complexion .\"                                                                                                                                                                                                                                                                                                                                                                                                      0.533736\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "CHAPTER XIX      0.998903\n",
      "CHAPTER XVIII    0.998903\n",
      "CHAPTER XVIII    0.998903\n",
      "CHAPTER XV       0.998903\n",
      "CHAPTER XV       0.998903\n",
      "CHAPTER XV       0.998903\n",
      "CHAPTER XVIII    0.998903\n",
      "CHAPTER XVII     0.998034\n",
      "CHAPTER XVII     0.998034\n",
      "CHAPTER XII      0.998021\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "\" Ah !     0.992908\n",
      "\" Ah !     0.992908\n",
      "\" Ah !     0.992908\n",
      "\" Ah !     0.992908\n",
      "\" Ah !     0.992908\n",
      "\" Ah !     0.992908\n",
      "\" Ah !     0.992908\n",
      "\" Ah !     0.992908\n",
      "\" Ah !     0.992908\n",
      "\" Ah !\"    0.992908\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "\" No , no ,\" said Emma , \" it will not reckon low .                                                             0.632497\n",
      "Nobody had any information to give ; and , after a few more wonderings , Emma said ,                            0.630740\n",
      "\" Well ,\" said Emma , \" there is no disputing about taste . At least you admire her except her complexion .\"    0.622127\n",
      "Emma had done .                                                                                                 0.599176\n",
      "\" Emma !\"                                                                                                       0.599176\n",
      "\" My Emma !\"                                                                                                    0.599176\n",
      "Emma wondered on what , of all the medley , she would fix .                                                     0.594368\n",
      "\" And I do envy him , Emma .                                                                                    0.592797\n",
      "\" Emma ,\" said she , \" this paper is worse than I expected .                                                    0.585763\n",
      "\" No ,\" said Emma , laughing ; \" but perhaps there might be some made to his coming back again .                0.550441\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_train_lsa = lsa.fit(X_train_tfidf)\n",
    "X_test_pred = X_train_lsa.transform(X_test_tfidf)\n",
    "\n",
    "paras_by_component=pd.DataFrame(X_test_pred,index=X_test)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, this model identifies similar sentences for components 0 through 4. Component 0 is all exclamations including \"Oh!\" Component 1 is all dialogue involving Mr.'s and Mrs.'s from Emmas circle of acquaintances. Component 2 is all chapter titles. Component 3 is all exclamations including \"Ah!\" Component 4 is all short dialogue directed at or coming from Emma, usually with a slight negative tone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drill 1: Tweaking tf-idf\n",
    "\n",
    "Go back up to the code where we originally translated the text from words to numbers. There are a lot of decision-points here, from the stop list to the thresholds for inclusion and exclusion, and many others as well. We also didn't integrate spaCy, and so don't have info on lemmas or Named Entities. Change things up a few times and see how that affects the results of the LSA. Write up your observations and share them with your mentor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I want to add spaCy information on lemmas to simplify the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[ Emma by Jane Austen 1816 ]', 'VOLUME I', 'CHAPTER I', 'Emma Woodhouse , handsome , clever , and rich , with a comfortable home and happy disposition , seemed to unite some of the best blessings of existence ; and had lived nearly twenty - one years in the world with very little to distress or vex her .', \"She was the youngest of the two daughters of a most affectionate , indulgent father ; and had , in consequence of her sister ' s marriage , been mistress of his house from a very early period .\", \"Sixteen years had Miss Taylor been in Mr . Woodhouse ' s family , less as a governess than a friend , very fond of both daughters , but particularly of Emma .\", \"The real evils , indeed , of Emma ' s situation were the power of having rather too much her own way , and a disposition to think a little too well of herself ; these were the disadvantages which threatened alloy to her many enjoyments .\", 'Sorrow came  a gentle sorrow  but not at all in the shape of any disagreeable consciousness . Miss Taylor married .', 'The event had every promise of happiness for her friend .', 'How was she to bear the change ? It was true that her friend was going only half a mile from them ; but Emma was aware that great must be the difference between a Mrs . Weston , only half a mile from them , and a Miss Taylor in the house ; and with all her advantages , natural and domestic , she was now in great danger of suffering from intellectual solitude .']\n",
      "['emma jane austen 1816', 'volume -PRON-', 'chapter -PRON-', 'emma woodhouse handsome clever rich comfortable home happy disposition unite well blessing existence live nearly year world little distress vex', '-PRON- young daughter affectionate indulgent father consequence sister s marriage mistress house early period', 'sixteen year miss taylor mr woodhouse s family governess friend fond daughter particularly emma', 'the real evil emma s situation power have way disposition think little disadvantage threaten alloy enjoyment', 'sorrow come   gentle sorrow   shape disagreeable consciousness miss taylor marry', 'the event promise happiness friend', 'how bear change -PRON- true friend go half mile emma aware great difference mrs weston half mile miss taylor house advantage natural domestic great danger suffer intellectual solitude']\n"
     ]
    }
   ],
   "source": [
    "spacy_paras = []\n",
    "\n",
    "for para in emma_paras:\n",
    "    spacy_paras.append(nlp(para))\n",
    "\n",
    "lemma_paras = []\n",
    "for para in spacy_paras:\n",
    "    lemma_para = [token.lemma_\n",
    "                  for token in para\n",
    "                  if (\n",
    "                      not token.is_punct\n",
    "                      and not token.is_stop\n",
    "                  )]\n",
    "    lemma_para = ' '.join(lemma_para)\n",
    "    lemma_paras.append(lemma_para)\n",
    "\n",
    "print(emma_paras[:10])    \n",
    "print(lemma_paras[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to use the same parameters as earlier but test them on the lemma-updated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 2371\n",
      "Vectorized features: 1539\n",
      "Original sentence: a minute complete present trial\n",
      "['abbey', 'able', 'abroad', 'absence', 'absent']\n",
      "Tf_idf vector: {'minute': 0.5433044653685889, 'present': 0.53946685074649903, 'complete': 0.64326959733552336}\n",
      "Component 0:\n",
      "oh    0.984798\n",
      "oh    0.984798\n",
      "oh    0.984798\n",
      "oh    0.984798\n",
      "oh    0.984798\n",
      "oh    0.984798\n",
      "oh    0.984798\n",
      "oh    0.984798\n",
      "oh    0.984798\n",
      "oh    0.984798\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "must -PRON-                      0.815864\n",
      "-PRON-                           0.815864\n",
      "-PRON-                           0.815864\n",
      "-PRON-                           0.815864\n",
      "not -PRON-                       0.815864\n",
      "as   -PRON-                      0.815864\n",
      "-PRON- go kingston               0.815864\n",
      "-PRON-                           0.815864\n",
      "-PRON- ungrateful                0.815864\n",
      "-PRON- forget -PRON- acquaint    0.793930\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "as   -PRON-                                    0.457535\n",
      "not -PRON-                                     0.457535\n",
      "-PRON- ungrateful                              0.457535\n",
      "-PRON-                                         0.457535\n",
      "must -PRON-                                    0.457535\n",
      "-PRON- go kingston                             0.457535\n",
      "-PRON-                                         0.457535\n",
      "-PRON-                                         0.457535\n",
      "-PRON-                                         0.457535\n",
      "-PRON- display wealth pomp king lords earth    0.445680\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "chapter xix      0.988056\n",
      "chapter xv       0.988056\n",
      "chapter xviii    0.988056\n",
      "chapter xviii    0.988056\n",
      "chapter xviii    0.988056\n",
      "chapter xv       0.988056\n",
      "chapter xv       0.988056\n",
      "chapter xiii     0.987280\n",
      "chapter xiii     0.987280\n",
      "chapter xvii     0.987259\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "emma                                                0.731983\n",
      "emma                                                0.731983\n",
      "emma repent condescension go coles                  0.692489\n",
      "nobody information wondering emma say               0.640314\n",
      "no say emma reckon low                              0.638385\n",
      "emma danger forget                                  0.617045\n",
      "emma eager superior treasure                        0.611069\n",
      "well say emma dispute taste at admire complexion    0.598782\n",
      "emma deny aloud agree private                       0.561787\n",
      "-PRON- emma                                         0.542311\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def change_vectorizer (df, maxdf, mindf):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_df=maxdf,\n",
    "                                 min_df=mindf,\n",
    "                                 stop_words='english', \n",
    "                                 lowercase=True,\n",
    "                                 use_idf=True,\n",
    "                                 norm=u'l2',\n",
    "                                 smooth_idf=True\n",
    "                                )\n",
    "    \n",
    "    df_tfidf = vectorizer.fit_transform(df)\n",
    "    print(f'Original features: {len(df)}')\n",
    "    print(f'Vectorized features: {df_tfidf.get_shape()[1]}')\n",
    "    \n",
    "    X_train, X_test= train_test_split(df, test_size=0.4, random_state=0)\n",
    "    X_train_tfidf, X_test_tfidf= train_test_split(df_tfidf, test_size=0.4, random_state=0)\n",
    "    X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "    n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "    tfidf_bypara = [{} for _ in range(0,n)]\n",
    "    \n",
    "    terms = vectorizer.get_feature_names()\n",
    "    \n",
    "    for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "        tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "    print('Original sentence:', X_train[5])\n",
    "    print(terms[:5])\n",
    "    print('Tf_idf vector:', tfidf_bypara[5])\n",
    "\n",
    "    X_train_lsa = lsa.fit(X_train_tfidf)\n",
    "    X_test_pred = X_train_lsa.transform(X_test_tfidf)\n",
    "\n",
    "    paras_by_component=pd.DataFrame(X_test_pred,index=X_test)\n",
    "    for i in range(5):\n",
    "        print('Component {}:'.format(i))\n",
    "        print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])\n",
    "    \n",
    "change_vectorizer(lemma_paras, 0.5,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component 0 is the same as earlier, lemma or not. This is a one word sentence which cannot be lemmatized any further. Component 1, which was previously a lot of dialogue refererring to Named Entities with varying vector values around 0.55, is now made up of short, simple sentences with lots of me's, you's, him's, etc (values more consistently around 0.81). Interestingly, Component 2 is made up of many of the same sentences as earlier but with much lower values. Component 3 is all chapter titles (cannot be lemmatized) and Component 4 is medium-length sentences involving Emma.<br>\n",
    "\n",
    "Let's see what happens when we change the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 2371\n",
      "Vectorized features: 375\n",
      "Original sentence: a minute complete present trial\n",
      "['abbey', 'able', 'absolutely', 'accept', 'account']\n",
      "Tf_idf vector: {'minute': 0.70960848901438356, 'present': 0.70459619096240045}\n",
      "Component 0:\n",
      "-PRON-                                         0.870795\n",
      "no   -PRON- worth                              0.870795\n",
      "-PRON- display wealth pomp king lords earth    0.870795\n",
      "-PRON- sick prosperity indulgence              0.870795\n",
      "volume -PRON-                                  0.870795\n",
      "-PRON-                                         0.870795\n",
      "as   -PRON-                                    0.870795\n",
      "-PRON-                                         0.870795\n",
      "-PRON- choose employ                           0.870795\n",
      "not -PRON-                                     0.870795\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "oh    0.998663\n",
      "oh    0.998663\n",
      "oh    0.998663\n",
      "oh    0.998663\n",
      "oh    0.998663\n",
      "oh    0.998663\n",
      "oh    0.998663\n",
      "oh    0.998663\n",
      "oh    0.998663\n",
      "oh    0.998663\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "emma repent condescension go coles                                                                                                                                                                                                            0.562373\n",
      "emma                                                                                                                                                                                                                                          0.562373\n",
      "emma                                                                                                                                                                                                                                          0.562373\n",
      "emma eager superior treasure                                                                                                                                                                                                                  0.557099\n",
      "the event favourable mr woodhouse emma                                                                                                                                                                                                        0.554988\n",
      "emma deny aloud agree private                                                                                                                                                                                                                 0.540969\n",
      "after tea mr mrs weston mr elton sit mr woodhouse card                                                                                                                                                                                        0.525232\n",
      "frank turn instantly emma claim promise boast engage man father look perfect approbation   appear mrs weston want dance mrs elton business help persuade pretty soon mr weston mrs elton lead way mr frank churchill miss woodhouse follow    0.516622\n",
      "well mrs weston say emma triumphantly leave mr knightley s marry jane fairfax                                                                                                                                                                 0.516342\n",
      "in walk emma mr weston find assemble view immediately perceive mr knightley harriet distinct rest quietly lead way                                                                                                                            0.512203\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "chapter xix      0.991535\n",
      "chapter xvii     0.991535\n",
      "chapter xviii    0.991535\n",
      "chapter vii      0.991535\n",
      "chapter xv       0.991535\n",
      "chapter xviii    0.991535\n",
      "chapter iv       0.991535\n",
      "chapter xiii     0.991535\n",
      "chapter xviii    0.991535\n",
      "chapter xiii     0.991535\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "emma                                                      0.707664\n",
      "emma repent condescension go coles                        0.707664\n",
      "emma                                                      0.707664\n",
      "emma eager superior treasure                              0.666751\n",
      "emma deny aloud agree private                             0.660053\n",
      "such imagination cross -PRON- emma occur consideration    0.581304\n",
      "and -PRON- envy emma                                      0.581304\n",
      "-PRON- emma                                               0.581304\n",
      "emma danger forget                                        0.561847\n",
      "well say emma dispute taste at admire complexion          0.542938\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "change_vectorizer(lemma_paras, 0.5,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raising the min_df to 10 makes the algorithm group the sentences less accurately. This means that the algorithm only takes into account words that appear at least 10 times. Some of the components now have more variability in sentence size and sentiment. Especially the last component which is only linked by the fact that the words \"emma\" is in all sentences. Emma must be one of the most frequently used words in the whole book, so it is taking only this word into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 2371\n",
      "Vectorized features: 1538\n",
      "Original sentence: a minute complete present trial\n",
      "['abbey', 'able', 'abroad', 'absence', 'absent']\n",
      "Tf_idf vector: {'minute': 0.5433044653685889, 'present': 0.53946685074649903, 'complete': 0.64326959733552336}\n",
      "Component 0:\n",
      "oh    0.999124\n",
      "oh    0.999124\n",
      "oh    0.999124\n",
      "oh    0.999124\n",
      "oh    0.999124\n",
      "oh    0.999124\n",
      "oh    0.999124\n",
      "oh    0.999124\n",
      "oh    0.999124\n",
      "oh    0.999124\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "frank turn instantly emma claim promise boast engage man father look perfect approbation   appear mrs weston want dance mrs elton business help persuade pretty soon mr weston mrs elton lead way mr frank churchill miss woodhouse follow    0.580826\n",
      "well mrs weston say emma triumphantly leave mr knightley s marry jane fairfax                                                                                                                                                                 0.570426\n",
      "-PRON- person -PRON- think month s end say mr knightley degree vexation emma immediately talk comprehend angry                                                                                                                                0.547622\n",
      "the result distress voluntary cheerful consent daughter presume hope moment able fix wed day   mr elton call month marriage mr mrs robert martin join hand mr knightley miss woodhouse                                                        0.538715\n",
      "mr knightley fact people fault emma woodhouse tell particularly agreeable emma know father suspect circumstance think perfect body                                                                                                            0.538519\n",
      "in walk emma mr weston find assemble view immediately perceive mr knightley harriet distinct rest quietly lead way                                                                                                                            0.529663\n",
      "nobody information wondering emma say                                                                                                                                                                                                         0.524148\n",
      "after tea mr mrs weston mr elton sit mr woodhouse card                                                                                                                                                                                        0.523391\n",
      "mrs weston s manner say emma particularly good                                                                                                                                                                                                0.515915\n",
      "-PRON- think foolish intimacy say mr knightley presently -PRON- keep thought -PRON- perceive unfortunate harriet                                                                                                                              0.514092\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "chapter xix      0.998945\n",
      "chapter xv       0.998945\n",
      "chapter xviii    0.998945\n",
      "chapter xv       0.998945\n",
      "chapter xviii    0.998945\n",
      "chapter xv       0.998945\n",
      "chapter xviii    0.998945\n",
      "chapter xvii     0.998147\n",
      "chapter xvii     0.998147\n",
      "chapter vi       0.998143\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "no say emma reckon low                                    0.603552\n",
      "nobody information wondering emma say                     0.599149\n",
      "emma                                                      0.588051\n",
      "-PRON- emma                                               0.588051\n",
      "emma                                                      0.588051\n",
      "and -PRON- envy emma                                      0.583108\n",
      "well say emma dispute taste at admire complexion          0.566635\n",
      "emma repent condescension go coles                        0.548364\n",
      "such imagination cross -PRON- emma occur consideration    0.513341\n",
      "emma eager superior treasure                              0.498684\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "ah    0.979633\n",
      "ah    0.979633\n",
      "ah    0.979633\n",
      "ah    0.979633\n",
      "ah    0.979633\n",
      "ah    0.979633\n",
      "ah    0.979633\n",
      "ah    0.979633\n",
      "ah    0.979633\n",
      "ah    0.979633\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "change_vectorizer(lemma_paras, 0.2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowering the max_df from 0.5 to 0.2 does not seem to change results drastically. This means that less terms are being taken into account, and the algorithm is grouping based on more unique words. In particular, I am noticing a lot of adverbs in Component 1. In component 3, we still see variance in sentiment and length. 0.2 might be too low to generate robust results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 2371\n",
      "Vectorized features: 374\n",
      "Original sentence: a minute complete present trial\n",
      "['abbey', 'able', 'absolutely', 'accept', 'account']\n",
      "Tf_idf vector: {'minute': 0.70960848901438356, 'present': 0.70459619096240045}\n",
      "Component 0:\n",
      "oh           0.996427\n",
      "oh           0.996427\n",
      "oh           0.996427\n",
      "oh           0.996427\n",
      "oh           0.996427\n",
      "oh           0.996427\n",
      "oh           0.996427\n",
      "oh           0.996427\n",
      "-PRON- oh    0.996427\n",
      "oh           0.996427\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "well say emma dispute taste at admire complexion                                                                                                                                                                                              0.589593\n",
      "nobody information wondering emma say                                                                                                                                                                                                         0.589593\n",
      "and forget matt joy say emma considerable   -PRON- match                                                                                                                                                                                      0.583355\n",
      "no say emma reckon low                                                                                                                                                                                                                        0.565253\n",
      "well mrs weston say emma triumphantly leave mr knightley s marry jane fairfax                                                                                                                                                                 0.554042\n",
      "frank turn instantly emma claim promise boast engage man father look perfect approbation   appear mrs weston want dance mrs elton business help persuade pretty soon mr weston mrs elton lead way mr frank churchill miss woodhouse follow    0.536741\n",
      "-PRON- think foolish intimacy say mr knightley presently -PRON- keep thought -PRON- perceive unfortunate harriet                                                                                                                              0.525473\n",
      "mr knightley fact people fault emma woodhouse tell particularly agreeable emma know father suspect circumstance think perfect body                                                                                                            0.525229\n",
      "in walk emma mr weston find assemble view immediately perceive mr knightley harriet distinct rest quietly lead way                                                                                                                            0.522899\n",
      "the event favourable mr woodhouse emma                                                                                                                                                                                                        0.516919\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "chapter xiv      1.0\n",
      "chapter xviii    1.0\n",
      "chapter xvii     1.0\n",
      "chapter vii      1.0\n",
      "chapter xv       1.0\n",
      "chapter xii      1.0\n",
      "chapter vi       1.0\n",
      "chapter xix      1.0\n",
      "chapter xviii    1.0\n",
      "chapter xvii     1.0\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "and -PRON- envy emma                                      0.688314\n",
      "emma                                                      0.688314\n",
      "emma repent condescension go coles                        0.688314\n",
      "such imagination cross -PRON- emma occur consideration    0.688314\n",
      "emma                                                      0.688314\n",
      "-PRON- emma                                               0.688314\n",
      "emma eager superior treasure                              0.644641\n",
      "emma deny aloud agree private                             0.644501\n",
      "well say emma dispute taste at admire complexion          0.556449\n",
      "nobody information wondering emma say                     0.556449\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "-PRON- aim wound harriet say                                                 0.591585\n",
      "-PRON- secret -PRON- conclude say                                            0.572830\n",
      "upon honour say seriously                                                    0.572830\n",
      "now say harriet recollect                                                    0.556427\n",
      "yes say harriet earnestly well                                               0.496425\n",
      "this present campbells say pianoforte kindly give                            0.491609\n",
      "how deceive -PRON- protest think seriously harriet                           0.489444\n",
      "well say try recover circumstance -PRON- think half day -PRON- comprehend    0.425326\n",
      "-PRON- give miss smith require say graceful easy                             0.425167\n",
      "there say reply harriet solemnly                                             0.420972\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "change_vectorizer(lemma_paras, 0.2,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping the max_df at a low 0.2 and raising the min_df to 10 leaves most of the groups the same except Component 4, which now has several mentions of the word \"harriet\" and \"say\". Many of these sentences have a positive sentiment, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we see how lemmatizing the sentences and tinkering with the hyperparameters affects the similarity, let's try using Named Entity Recognition. We will remove the named entities from the sentences, which I assume will match sentences based more on structure and sentiment as opposed to the specific characters they refer to. <br>\n",
    "\n",
    "We will create a function that uses a list comprehension to: <br>\n",
    "- Tokenize and add POS tag to each word in the sentence\n",
    "- Filter out all NNP words from the sentence\n",
    "- Remove the POS tag so we are just left with the words\n",
    "- Join words back into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('[', 'NNS'), ('Emma', 'NNP'), ('by', 'IN'), ('Jane', 'NNP'), ('Austen', 'NNP'), ('1816', 'CD'), (']', 'NN')], [('VOLUME', 'NNP'), ('I', 'PRP')], [('CHAPTER', 'NN'), ('I', 'PRP')], [('Emma', 'NNP'), ('Woodhouse', 'NNP'), (',', ','), ('handsome', 'NN'), (',', ','), ('clever', 'NN'), (',', ','), ('and', 'CC'), ('rich', 'JJ'), (',', ','), ('with', 'IN'), ('a', 'DT'), ('comfortable', 'JJ'), ('home', 'NN'), ('and', 'CC'), ('happy', 'JJ'), ('disposition', 'NN'), (',', ','), ('seemed', 'VBD'), ('to', 'TO'), ('unite', 'VB'), ('some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('blessings', 'NNS'), ('of', 'IN'), ('existence', 'NN'), (';', ':'), ('and', 'CC'), ('had', 'VBD'), ('lived', 'VBN'), ('nearly', 'RB'), ('twenty', 'JJ'), ('-', ':'), ('one', 'CD'), ('years', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('world', 'NN'), ('with', 'IN'), ('very', 'RB'), ('little', 'JJ'), ('to', 'TO'), ('distress', 'VB'), ('or', 'CC'), ('vex', 'VB'), ('her', 'PRP'), ('.', '.')], [('She', 'PRP'), ('was', 'VBD'), ('the', 'DT'), ('youngest', 'JJS'), ('of', 'IN'), ('the', 'DT'), ('two', 'CD'), ('daughters', 'NNS'), ('of', 'IN'), ('a', 'DT'), ('most', 'RBS'), ('affectionate', 'JJ'), (',', ','), ('indulgent', 'JJ'), ('father', 'NN'), (';', ':'), ('and', 'CC'), ('had', 'VBD'), (',', ','), ('in', 'IN'), ('consequence', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('sister', 'NN'), (\"'\", \"''\"), ('s', 'JJ'), ('marriage', 'NN'), (',', ','), ('been', 'VBN'), ('mistress', 'NN'), ('of', 'IN'), ('his', 'PRP$'), ('house', 'NN'), ('from', 'IN'), ('a', 'DT'), ('very', 'RB'), ('early', 'JJ'), ('period', 'NN'), ('.', '.')]]\n",
      "\n",
      "\n",
      "['[ by 1816 ]', 'I', 'CHAPTER I', ', handsome , clever , and rich , with a comfortable home and happy disposition , seemed to unite some of the best blessings of existence ; and had lived nearly twenty - one years in the world with very little to distress or vex her .', \"She was the youngest of the two daughters of a most affectionate , indulgent father ; and had , in consequence of her sister ' s marriage , been mistress of his house from a very early period .\"]\n"
     ]
    }
   ],
   "source": [
    "def pos_process(document):\n",
    "    text = [nltk.word_tokenize(sentence) for sentence in document]\n",
    "    sentences = [nltk.pos_tag(sentence) for sentence in text]\n",
    "    print(sentences[:5])\n",
    "    print('\\n')\n",
    "    \n",
    "    new_sentences = [[x[0] for x in sentence if not x[1] == 'NNP'] for sentence in sentences]\n",
    "    joined_sentences = [' '.join(sentence) for sentence in new_sentences]\n",
    " \n",
    "    print(joined_sentences[:5])\n",
    "    return joined_sentences\n",
    "    \n",
    "nameless_emma = pos_process(emma_paras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, I will use the original hyperparameters on the nameless dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 2371\n",
      "Vectorized features: 1859\n",
      "Original sentence: A very few minutes more , however , completed the present trial .\n",
      "['_her_', '_home_', '_i_', '_is_', '_me_']\n",
      "Tf_idf vector: {'minutes': 0.71274503103825837, 'present': 0.70142321085794701}\n",
      "Component 0:\n",
      "`` Oh !       0.999311\n",
      "`` Oh !       0.999311\n",
      "`` Oh no !    0.999311\n",
      "`` Oh !       0.999311\n",
      "`` Oh !       0.999311\n",
      "`` Oh !       0.999311\n",
      "`` Oh !       0.999311\n",
      "`` Oh !       0.999311\n",
      "`` Oh !       0.999311\n",
      "`` Oh !       0.999311\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "CHAPTER          0.999909\n",
      "CHAPTER          0.999909\n",
      "CHAPTER          0.999909\n",
      "CHAPTER          0.999909\n",
      "CHAPTER          0.999909\n",
      "CHAPTER XIX      0.999909\n",
      "CHAPTER          0.999909\n",
      "CHAPTER XVIII    0.999909\n",
      "CHAPTER XV       0.999909\n",
      "CHAPTER XVIII    0.999909\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "`` It is and , '' said . .                                                                                       0.783058\n",
      "Nobody had any information to give ; and , after a few more wonderings , said ,                                  0.766852\n",
      "`` Well , '' said , `` there is no disputing about taste . At least you admire her except her complexion . ''    0.765699\n",
      "`` and . are to be in town again by midsummer , '' said .                                                        0.765619\n",
      "`` You will get nothing to the purpose from , '' said .                                                          0.764302\n",
      "`` They aimed at wounding more than , '' said he .                                                               0.756670\n",
      "`` No , no , '' said , `` it will not reckon low .                                                               0.749681\n",
      "`` Your parish there was small , '' said .                                                                       0.736180\n",
      "`` Now , '' said , `` you _must_ recollect . ''                                                                  0.735926\n",
      "`` It is to be a secret , I conclude , '' said he .                                                              0.730420\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "`` Ah !       0.99974\n",
      "`` Ah !       0.99974\n",
      "`` Ah !       0.99974\n",
      "`` Ah ! ''    0.99974\n",
      "`` Ah !       0.99974\n",
      "`` Ah ! ''    0.99974\n",
      "`` Ah !       0.99974\n",
      "`` Ah !       0.99974\n",
      "`` Ah !       0.99974\n",
      "`` Ah !       0.99974\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "`` So they will , my dear .                                                                                                                                                                    0.731781\n",
      "`` , , my dear , where are you ? Here is your tippet .                                                                                                                                         0.731781\n",
      "`` Dear me ! How should I ever have borne it !                                                                                                                                                 0.728791\n",
      "`` Middling , my dear ; I can not compliment you .                                                                                                                                             0.728015\n",
      "`` Dear affectionate creature ! _You_ banished to - ! _You_ confined to the society of the illiterate and vulgar all your life !                                                               0.680145\n",
      "`` My dear , a vast deal may be done by those who dare to act .                                                                                                                                0.607747\n",
      "`` Why , pretty well , my dear , upon the whole .                                                                                                                                              0.594737\n",
      "`` I do not know , my dear .                                                                                                                                                                   0.559242\n",
      "Now I say , my dear , in _our_ case , for , read mum !                                                                                                                                         0.556507\n",
      "`` Here comes this dear old beau of mine , I protest ! Only think of his gallantry in coming away before the other men ! what a dear creature he is ; I assure you I like him excessively .    0.555888\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "change_vectorizer(nameless_emma, 0.5,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't get much information with so many of these top components being populated by short exclamations. For the two components that do give us information, the clusters seem to be centered around the word \"said\" and \"dear\". It seems that using \"dear\" often might be indicative of a certain character's speech dialect, and this would make these sentences very similar because they are being spoken in the same style. However, \"said\" is not a very good indicator for similarity, for it simply denotes dialogue. Let's see if changing the hyperparameters can keep Component 4 the same but change Component 2. To do this, I will raise the max_df so that common words like \"said\" are no longer considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 2371\n",
      "Vectorized features: 1859\n",
      "Original sentence: A very few minutes more , however , completed the present trial .\n",
      "['_her_', '_home_', '_i_', '_is_', '_me_']\n",
      "Tf_idf vector: {'minutes': 0.71274503103825837, 'present': 0.70142321085794701}\n",
      "Component 0:\n",
      "`` Oh !    0.999309\n",
      "`` Oh !    0.999309\n",
      "`` Oh !    0.999309\n",
      "`` Oh !    0.999309\n",
      "`` Oh !    0.999309\n",
      "`` Oh !    0.999309\n",
      "`` Oh !    0.999309\n",
      "`` Oh !    0.999309\n",
      "`` Oh !    0.999309\n",
      "`` Oh !    0.999309\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "CHAPTER XVIII    0.999891\n",
      "CHAPTER XV       0.999891\n",
      "CHAPTER XV       0.999891\n",
      "CHAPTER XVIII    0.999891\n",
      "CHAPTER XVIII    0.999891\n",
      "CHAPTER XV       0.999891\n",
      "CHAPTER          0.999891\n",
      "CHAPTER          0.999891\n",
      "CHAPTER          0.999891\n",
      "CHAPTER XIX      0.999891\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "`` It is and , '' said . .                                                                                       0.783177\n",
      "Nobody had any information to give ; and , after a few more wonderings , said ,                                  0.768155\n",
      "`` and . are to be in town again by midsummer , '' said .                                                        0.767159\n",
      "`` You will get nothing to the purpose from , '' said .                                                          0.764729\n",
      "`` Well , '' said , `` there is no disputing about taste . At least you admire her except her complexion . ''    0.764172\n",
      "`` They aimed at wounding more than , '' said he .                                                               0.756985\n",
      "`` No , no , '' said , `` it will not reckon low .                                                               0.743224\n",
      "`` Now , '' said , `` you _must_ recollect . ''                                                                  0.734240\n",
      "`` It is to be a secret , I conclude , '' said he .                                                              0.732524\n",
      "`` Your parish there was small , '' said .                                                                       0.711377\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "`` Ah !       0.999744\n",
      "`` Ah !       0.999744\n",
      "`` Ah ! ''    0.999744\n",
      "`` Ah !       0.999744\n",
      "`` Ah !       0.999744\n",
      "`` Ah ! ''    0.999744\n",
      "`` Ah !       0.999744\n",
      "`` Ah !       0.999744\n",
      "`` Ah !       0.999744\n",
      "`` Ah !       0.999744\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "`` So they will , my dear .                                                                                                                                                                    0.731738\n",
      "`` , , my dear , where are you ? Here is your tippet .                                                                                                                                         0.731738\n",
      "`` Dear me ! How should I ever have borne it !                                                                                                                                                 0.729787\n",
      "`` Middling , my dear ; I can not compliment you .                                                                                                                                             0.729666\n",
      "`` Dear affectionate creature ! _You_ banished to - ! _You_ confined to the society of the illiterate and vulgar all your life !                                                               0.663903\n",
      "`` My dear , a vast deal may be done by those who dare to act .                                                                                                                                0.606931\n",
      "`` Why , pretty well , my dear , upon the whole .                                                                                                                                              0.590376\n",
      "`` I do not know , my dear .                                                                                                                                                                   0.559571\n",
      "Now I say , my dear , in _our_ case , for , read mum !                                                                                                                                         0.558320\n",
      "`` Here comes this dear old beau of mine , I protest ! Only think of his gallantry in coming away before the other men ! what a dear creature he is ; I assure you I like him excessively .    0.549020\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "change_vectorizer(nameless_emma, .9,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter how high I raise the threshold, the Components do not change. Perhaps dialogue described by the word \"said\" is less common than I thought. Interestingly, when I lower the max_df threshold dramatically, the word \"dear\" takes over every component (not demonstrated here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Changing the structure of the datasets and the hyperparameters in the vectorizer certainly changes how datasets can be grouped by similarity. However, similarity of natural language is a highly subjective concept, and different types of similarity may be good for different situations. Since this is an unsupervised prompt, there is no objective way to assess performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
