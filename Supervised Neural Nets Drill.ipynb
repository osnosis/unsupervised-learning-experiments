{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import ensemble\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data comes from a public dataset by the Museum of Modern Art in NYC. It includes information on their art collection, including the names of their pieces, artist, date of acquisition, identification information such as Accession Number and ObjectID, and physical attributes such as length and width. In this experiment, I will Now, use multi-layer perceptron modeling (MLP) to see if I can classify the department a piece should go into using everything but the department name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data and look at columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artworks = pd.read_csv('https://media.githubusercontent.com/media/MuseumofModernArt/collection/master/Artworks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Artist', 'ConstituentID', 'ArtistBio', 'Nationality',\n",
       "       'BeginDate', 'EndDate', 'Gender', 'Date', 'Medium', 'Dimensions',\n",
       "       'CreditLine', 'AccessionNumber', 'Classification', 'Department',\n",
       "       'DateAcquired', 'Cataloged', 'ObjectID', 'URL', 'ThumbnailURL',\n",
       "       'Circumference (cm)', 'Depth (cm)', 'Diameter (cm)', 'Height (cm)',\n",
       "       'Length (cm)', 'Weight (kg)', 'Width (cm)', 'Seat Height (cm)',\n",
       "       'Duration (sec.)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artworks.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select columns. <br>\n",
    "Convert URLs to booleans. <br>\n",
    "Drop 'films' and other tricky rows. <br>\n",
    "Drop missing data. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artworks = artworks[['Artist', 'Nationality', 'Gender', 'Date', 'Department',\n",
    "                    'DateAcquired', 'URL', 'ThumbnailURL', 'Height (cm)', 'Width (cm)']]\n",
    "\n",
    "artworks['URL'] = artworks['URL'].notnull()\n",
    "artworks['ThumbnailURL'] = artworks['ThumbnailURL'].notnull()\n",
    "\n",
    "artworks = artworks[artworks['Department']!='Film']\n",
    "artworks = artworks[artworks['Department']!='Media and Performance Art']\n",
    "artworks = artworks[artworks['Department']!='Fluxus Collection']\n",
    "\n",
    "artworks = artworks.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, look at data and check out what kind of features we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Date</th>\n",
       "      <th>Department</th>\n",
       "      <th>DateAcquired</th>\n",
       "      <th>URL</th>\n",
       "      <th>ThumbnailURL</th>\n",
       "      <th>Height (cm)</th>\n",
       "      <th>Width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Otto Wagner</td>\n",
       "      <td>(Austrian)</td>\n",
       "      <td>(Male)</td>\n",
       "      <td>1896</td>\n",
       "      <td>Architecture &amp; Design</td>\n",
       "      <td>1996-04-09</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>48.6000</td>\n",
       "      <td>168.9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Christian de Portzamparc</td>\n",
       "      <td>(French)</td>\n",
       "      <td>(Male)</td>\n",
       "      <td>1987</td>\n",
       "      <td>Architecture &amp; Design</td>\n",
       "      <td>1995-01-17</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>40.6401</td>\n",
       "      <td>29.8451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Emil Hoppe</td>\n",
       "      <td>(Austrian)</td>\n",
       "      <td>(Male)</td>\n",
       "      <td>1903</td>\n",
       "      <td>Architecture &amp; Design</td>\n",
       "      <td>1997-01-15</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>34.3000</td>\n",
       "      <td>31.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bernard Tschumi</td>\n",
       "      <td>()</td>\n",
       "      <td>(Male)</td>\n",
       "      <td>1980</td>\n",
       "      <td>Architecture &amp; Design</td>\n",
       "      <td>1995-01-17</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>50.8000</td>\n",
       "      <td>50.8000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emil Hoppe</td>\n",
       "      <td>(Austrian)</td>\n",
       "      <td>(Male)</td>\n",
       "      <td>1903</td>\n",
       "      <td>Architecture &amp; Design</td>\n",
       "      <td>1997-01-15</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>38.4000</td>\n",
       "      <td>19.1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Artist Nationality  Gender  Date             Department  \\\n",
       "0               Otto Wagner  (Austrian)  (Male)  1896  Architecture & Design   \n",
       "1  Christian de Portzamparc    (French)  (Male)  1987  Architecture & Design   \n",
       "2                Emil Hoppe  (Austrian)  (Male)  1903  Architecture & Design   \n",
       "3           Bernard Tschumi          ()  (Male)  1980  Architecture & Design   \n",
       "4                Emil Hoppe  (Austrian)  (Male)  1903  Architecture & Design   \n",
       "\n",
       "  DateAcquired   URL ThumbnailURL  Height (cm)  Width (cm)  \n",
       "0   1996-04-09  True         True      48.6000    168.9000  \n",
       "1   1995-01-17  True         True      40.6401     29.8451  \n",
       "2   1997-01-15  True         True      34.3000     31.8000  \n",
       "3   1995-01-17  True         True      50.8000     50.8000  \n",
       "4   1997-01-15  True         True      38.4000     19.1000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artworks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Artist           object\n",
       "Nationality      object\n",
       "Gender           object\n",
       "Date             object\n",
       "Department       object\n",
       "DateAcquired     object\n",
       "URL                bool\n",
       "ThumbnailURL       bool\n",
       "Height (cm)     float64\n",
       "Width (cm)      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artworks.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert date to numeric and create a new feature for the year the piece was acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artworks['DateAcquired'] = pd.to_datetime(artworks.DateAcquired)\n",
    "artworks['YearAcquired'] = artworks.DateAcquired.dt.year\n",
    "artworks['YearAcquired'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove multiple nationalities, genders, and artists. <br>\n",
    "Convert dates to start date, cutting down number of distinct examples. <br>\n",
    "Define experimental X dataframe and perform final column drops/NA drop. <br>\n",
    "Create dummy categories separately then concatenate them together. <br>\n",
    "Define Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artworks.loc[artworks['Gender'].str.contains('\\) \\('), 'Gender'] = '\\(multiple_persons\\)'\n",
    "artworks.loc[artworks['Nationality'].str.contains('\\) \\('), 'Nationality'] = '\\(multiple_nationalities\\)'\n",
    "artworks.loc[artworks['Artist'].str.contains(','), 'Artist'] = 'Multiple_Artists'\n",
    "\n",
    "artworks['Date'] = pd.Series(artworks.Date.str.extract(\n",
    "    '([0-9]{4})', expand=False))[:-1]\n",
    "\n",
    "X = artworks.drop(['Department', 'DateAcquired', 'Artist', 'Nationality', 'Date'], 1)\n",
    "\n",
    "artists = pd.get_dummies(artworks.Artist)\n",
    "nationalities = pd.get_dummies(artworks.Nationality)\n",
    "dates = pd.get_dummies(artworks.Date)\n",
    "\n",
    "X = pd.get_dummies(X, sparse=True)\n",
    "X = pd.concat([X, nationalities, dates], axis=1)\n",
    "\n",
    "Y = artworks.Department"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! We've done our prep, let's build the model. Neural networks are hugely computationally intensive. This may take several minutes to run.<br>\n",
    "\n",
    "Establish and fit the model with a single 1000 perceptron layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(1000,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(1000,))\n",
    "mlp.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71537805516252206"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prints & Illustrated Books    0.523811\n",
       "Photography                   0.225079\n",
       "Architecture & Design         0.112399\n",
       "Drawings                      0.103997\n",
       "Painting & Sculpture          0.034714\n",
       "Name: Department, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.value_counts()/len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.61088231,  0.67525923,  0.38750787,  0.57477224,  0.51463462])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(mlp, X, Y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drill: Playing with layers\n",
    "Now it's your turn. Using the space below, experiment with different hidden layer structures. You can try this on a subset of the data to improve runtime. See how things vary. See what seems to matter the most. Feel free to manipulate other parameters as well. It may also be beneficial to do some real feature selection work..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will create subsets to reduce the runtime and compare the full dataset to the reduced dataset for several configurations of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "art_50 = artworks.sample(frac=0.5)\n",
    "\n",
    "X_50 = art_50.drop(['Department', 'DateAcquired', 'Artist', 'Nationality', 'Date'], 1)\n",
    "\n",
    "artists = pd.get_dummies(art_50.Artist)\n",
    "nationalities = pd.get_dummies(art_50.Nationality)\n",
    "dates = pd.get_dummies(art_50.Date)\n",
    "\n",
    "X_50 = pd.get_dummies(X_50, sparse=True)\n",
    "X_50 = pd.concat([X_50, nationalities, dates], axis=1)\n",
    "\n",
    "Y_50 = art_50.Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "art_10 = artworks.sample(frac=0.1)\n",
    "\n",
    "X_10 = art_10.drop(['Department', 'DateAcquired', 'Artist', 'Nationality', 'Date'], 1)\n",
    "\n",
    "artists = pd.get_dummies(art_10.Artist)\n",
    "nationalities = pd.get_dummies(art_10.Nationality)\n",
    "dates = pd.get_dummies(art_10.Date)\n",
    "\n",
    "X_10 = pd.get_dummies(X_10, sparse=True)\n",
    "X_10 = pd.concat([X_10, nationalities, dates], axis=1)\n",
    "\n",
    "Y_10 = art_10.Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: [100, 4]\n",
      "Accuracy: 0.5238113697594635\n",
      "Cross Val Scores: [ 0.52376569  0.52379107  0.52381645  0.52384183  0.52384183]\n",
      "Cross Val Mean: 0.5238113714321069\n"
     ]
    }
   ],
   "source": [
    "def run_mlp(X,Y, sizes):\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(sizes))\n",
    "    mlp.fit(X, Y)\n",
    "    print(f'Hidden Layer Sizes: {sizes}')\n",
    "    print(f'Accuracy: {mlp.score(X, Y)}')\n",
    "    scores = cross_val_score(mlp, X, Y, cv=5)\n",
    "    print(f'Cross Val Scores: {scores}')\n",
    "    print(f'Cross Val Mean: {scores.mean()}')\n",
    "    \n",
    "run_mlp(X, Y, [100,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: [100, 4]\n",
      "Accuracy: 0.5587385885682166\n",
      "Cross Val Scores: [ 0.5253876   0.52543851  0.52543851  0.52543851  0.52549438]\n",
      "Cross Val Mean: 0.5254395018031999\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_50, Y_50, [100,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: [100, 4]\n",
      "Accuracy: 0.5198178118034693\n",
      "Cross Val Scores: [ 0.53775411  0.51961259  0.51986434  0.52011634  0.52013586]\n",
      "Cross Val Mean: 0.5227264274297059\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_10, Y_10, [100,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a network structure of [100,4], we can see that the accuracy of the network stays relatively the same across datasets. This is an interesting finding and not what I expected, since neural nets tend to need a lot of data. None of the networks are overfitting by much either. Perhaps the lack of data can be made up for by expanding the size of the hidden layers, so more analysis is being done despite having less data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: [10, 10, 10]\n",
      "Accuracy: 0.6611555831217413\n",
      "Cross Val Scores: [ 0.6008043   0.63475143  0.56626448  0.56590425  0.50702656]\n",
      "Cross Val Mean: 0.5438327899489892\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X, Y, [10,10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: [10, 10, 10]\n",
      "Accuracy: 0.6543717170934041\n",
      "Cross Val Scores: [ 0.62839147  0.64550829  0.59404981  0.60819847  0.64530826]\n",
      "Cross Val Mean: 0.6092878068959712\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_50, Y_50, [10,10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: [10, 10, 10]\n",
      "Accuracy: 0.600445779629809\n",
      "Cross Val Scores: [ 0.5285576   0.54915254  0.58187984  0.5681047   0.59679767]\n",
      "Cross Val Mean: 0.5660266526736795\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_10, Y_10, [10,10,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a minimal network structure of [10, 10, 10], we see that the accuracy drops from 0.66 to 0.65 to 0.60 as we reduce the dataset. The difference between the full dataset and the 50% dataset is not as big a difference as the 10% dataset. This shows us that although we see a benefit in reduced runtime, it may not be worth our while to reduce datasets in the future because the performance gets worse and worse in a non-linear manner. <br>\n",
    "\n",
    "I have read advice online that says beyond 2-3 hidden layers, there tends not to be a big increase in performance. Let's test that theory by running 5 layers with 10 perceptrons each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: [10, 10, 10, 10, 10]\n",
      "Accuracy: 0.5979396429748222\n",
      "Cross Val Scores: [ 0.59038713  0.61488516  0.53772351  0.55194805  0.5086257 ]\n",
      "Cross Val Mean: 0.5747460568866721\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X, Y, [10,10,10,10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: [10, 10, 10, 10, 10]\n",
      "Accuracy: 0.6595080728005738\n",
      "Cross Val Scores: [ 0.63653101  0.57466809  0.62176567  0.57554027  0.65354789]\n",
      "Cross Val Mean: 0.6170057564137159\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_50, Y_50, [10,10,10,10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: [10, 10, 10, 10, 10]\n",
      "Accuracy: 0.5893012888845819\n",
      "Cross Val Scores: [ 0.56389158  0.58014528  0.52374031  0.52544838  0.51868025]\n",
      "Cross Val Mean: 0.5407504068716287\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_10, Y_10, [10,10,10,10,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 layers of [10,10,10] gave scores of 0.66, 0.65, and 0.60. Increasing to 5 layers gave scores of 0.57, 0.62, and 0.54. This shows that adding more layers doesn't necessarily improve results. It seems like the size of the layers is more important. Here we see that the 50% reduced dataset actually performed the best, despite some overfitting. This finding would contradict the earlier finding that more data produces better results, but this finding also uses 5 small hidden layers, when standard practice is to use fewer larger layers. <br>\n",
    "\n",
    "Let's expand the size of the hidden layers, but decrease the number to the minimum (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: [150]\n",
      "Accuracy: 0.6175934719826333\n",
      "Cross Val Scores: [ 0.60797519  0.66547146  0.48122305  0.56503198  0.48245784]\n",
      "Cross Val Mean: 0.5385386661528683\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X, Y, [150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: [150]\n",
      "Accuracy: 0.5870757660922994\n",
      "Cross Val Scores: [ 0.62577519  0.64919081  0.55751526  0.59686016  0.6240791 ]\n",
      "Cross Val Mean: 0.5644374284034559\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_50, Y_50, [150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: [150]\n",
      "Accuracy: 0.561197790483574\n",
      "Cross Val Scores: [ 0.53436592  0.5598063   0.53246124  0.57198255  0.55409995]\n",
      "Cross Val Mean: 0.473447241477135\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_10, Y_10, [150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have our worst set of scores yet at 0.54, 0.56, and 0.47. Perhaps more than one layer is needed, or if there is only one layer, it must be much larger than this. <br>\n",
    "\n",
    "Lets see what happens when we continue with the minimum of 1 layer, but this time with a minimal size of 2. Will the score get even worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: [2]\n",
      "Accuracy: 0.5238113697594635\n",
      "Cross Val Scores: [ 0.52376569  0.52379107  0.52381645  0.52384183  0.52384183]\n",
      "Cross Val Mean: 0.5148167741133588\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X, Y, [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: [2]\n",
      "Accuracy: 0.5254394976062644\n",
      "Cross Val Scores: [ 0.5253876   0.5512162   0.55858126  0.52543851  0.52549438]\n",
      "Cross Val Mean: 0.5254395018031999\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_50, Y_50, [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: [2]\n",
      "Accuracy: 0.5198178118034693\n",
      "Cross Val Scores: [ 0.51936108  0.51961259  0.51986434  0.52011634  0.52013586]\n",
      "Cross Val Mean: 0.5286316009058842\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_10, Y_10, [2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With scores of 0.51, 0.53, and 0.53, these scores with hidden layers=[2] are comparable with the scores of hidden layers=[150]. Let's see when we keep the minimum of 1 layer, but expand the size drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: [1000]\n",
      "Accuracy: 0.7021979725931813\n",
      "Cross Val Scores: [ 0.58205339  0.6389185   0.58782769  0.57753441  0.45798604]\n",
      "Cross Val Mean: 0.5718116178346173\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X, Y, [1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: [1000]\n",
      "Accuracy: 0.533405694570969\n",
      "Cross Val Scores: [ 0.56560078  0.64550829  0.53638918  0.24915205  0.64521132]\n",
      "Cross Val Mean: 0.5423349798904916\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_50, Y_50, [1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer Sizes: [1000]\n",
      "Accuracy: 0.2944083729043512\n",
      "Cross Val Scores: [ 0.53194579  0.29539952  0.53682171  0.53756665  0.5346919 ]\n",
      "Cross Val Mean: 0.4872851115550634\n"
     ]
    }
   ],
   "source": [
    "run_mlp(X_10, Y_10, [1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we expanded the layer, the full dataset saw the accuracy increase to 0.7, which is the best so far, but it is also overfitting as we can see by the cross-val score of 0.57. Also, it took forever. The 50% reduced dataset was not overfitting, but the accuracy was reduced to 0.53. The 10% dataset was dramatically underfitting, which is interesting. This shows us that although large datasets are preferred, they may lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform some basic feature selection to see if we can improve this algorithm. Now that we are trying to make a robust algorithm (instead of just tinkering with variables) we will split our data into training and testing datasets to see the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_feat = feature_selection.SelectKBest()\n",
    "\n",
    "    pipe = pipeline.Pipeline([('scaler', preprocessing.StandardScaler()),\n",
    "                                 ('feat', top_feat),\n",
    "                                 ('clf', linear_model.LogisticRegression())])\n",
    "\n",
    "\n",
    "var = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "\n",
    "\n",
    "clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\"))),\n",
    "  ('classification', RandomForestClassifier())\n",
    "])\n",
    "clf.fit(X, y)\n",
    "\n",
    "#sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "#sel.fit_transform(X)\n",
    "\n",
    "#pipe = Pipeline([\n",
    "#    ('reduce_dim', PCA()),\n",
    "#    ('classify', LinearSVC())\n",
    "\n",
    "clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\"))),\n",
    "  ('classification', RandomForestClassifier())\n",
    "])\n",
    "clf.fit(X, y)\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('feat', VarianceThreshold(threshold=(.8 * (1 - .8)))),\n",
    "    ('mlp', MLPClassifier())\n",
    "])\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "# get the accuracy\n",
    "print accuracy_score(y_test, predicted)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('feat', VarianceThreshold(threshold=(.8 * (1 - .8)))),\n",
    "    ('mlp', MLPClassifier(hidden_layer_sizes=[10,10,10]))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feat__threshold': 0.15999999999999998, 'mlp__hidden_layer_sizes': [10, 10, 10]}\n",
      "0.54520786898\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('feat', VarianceThreshold()),\n",
    "    ('mlp', MLPClassifier())\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'feat__threshold': [(.8 * (1 - .8)),(.6 * (1 - .6)),(.4 * (1 - .4))],\n",
    "        'mlp__hidden_layer_sizes': [[10,10,10],[1000]]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=3, n_jobs=1, param_grid=param_grid)\n",
    "grid.fit(X_train, Y_train)\n",
    "print(grid.best_params_)\n",
    "prediction = grid.predict(X_test)\n",
    "print(accuracy_score(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a validation score of 0.55, feature selection via variance threshold did not drastically improve our results. We used the hidden layer structures that produced the 2 best scores from earlier in the notebook to test this. When using a hidden layer structure of [10,10,10] on the full dataset, the score hardly changed at all, remaining around 0.54. We will try one more method of feature reduction -- SelectKBest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 17  60  75 122 127 131] are constant.\n",
      "  UserWarning)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.631504990794\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('feat', SelectKBest(k=100)),\n",
    "    ('mlp', MLPClassifier(hidden_layer_sizes = [10,10,10]))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, Y_train)\n",
    "prediction = pipe.predict(X_test)\n",
    "print(accuracy_score(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! Using SelectKBest with 100 features, we were able to obtain our best score yet -- 0.63. This is still not great, but a significant improvement from the values around 0.54 that all the other models were obtaining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "I have experimented with hidden layer structures, dataset sizes, and feature selection to classify this set using neural nets. With the full feature dataset, I generally found that including all datapoints led to greater accuracy scores, but this was not always the case. Sometimes including all the datapoints led to overfitting, so I will make a note to be cautious about this in the future. When the structure was very small and minimal, I saw more consistency in results between the differently-sized datasets. <br>\n",
    "\n",
    "Given that there were so many categorical features conveying little information per feature, I thought that setting a variance threshold to reduce the feature set would be beneficial. However, even after iterating through multiple threshold values and hidden layer structure using GridSearch, I did not see an increase in performance. I did, however, see a significant increase in performance when using SelectKBest, a method I had never used before. I will continue using these methods as I learn more about data science."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
